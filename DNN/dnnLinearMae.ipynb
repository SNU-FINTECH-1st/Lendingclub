{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed18a53f-419e-4f75-936c-0a47bf0adcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder\n",
    "import pandas \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.functional import f1_score, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ecc6d-b02e-4db6-911e-279e4af3211f",
   "metadata": {},
   "source": [
    "# 데이터 핸들링 부분, ( 이 부분은 안써요 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a64f92e-6de8-47b4-b7f1-199c10256a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 로드\n",
    "df = pandas.read_csv(\"./lending_club_2020_train.csv\", low_memory=False)\n",
    "df['target'] = df['total_rec_prncp']/df['funded_amnt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "91ca141b-1b8e-4609-a17c-e7559bcc12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'target' 열을 레이블로 사용하고 나머지 열은 특성으로 사용\n",
    "y = df['target'].values  # 'target' 열을 레이블로 사용\n",
    "x = df.drop(columns=['target']).select_dtypes(include=[float, int])  # 'target' 열을 제외한 숫자형 열을 특성으로 사용\n",
    "\n",
    "# NaN 값이 있는 모든 행을 삭제합니다.\n",
    "x['target'] = y  # y를 다시 x에 추가하여 동시 정렬을 보장합니다.\n",
    "x_clean = x.dropna()  # NaN 값이 있는 행 삭제\n",
    "\n",
    "# NaN을 제거한 후 x와 y를 분리\n",
    "y_clean = x_clean['target'].values\n",
    "x_clean = x_clean.drop(columns=['target']).values\n",
    "\n",
    "\n",
    "# 데이터 스플릿 (train 50%, validation 30%, test 20%)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_clean, y_clean, test_size=0.5, random_state=42)  # 50% train, 50% temp\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, test_size=0.4, random_state=42)  # 30% valid, 20% test\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "# 텐서로 변환\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # 크기를 (batch_size, 1)로 맞추기 위해 unsqueeze 추가\n",
    "x_valid = torch.tensor(x_valid, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.float32).unsqueeze(1)  # 크기를 (batch_size, 1)로 맞추기 위해 unsqueeze 추가\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)  # 크기를 (batch_size, 1)로 맞추기 위해 unsqueeze 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d411853-8a15-4c23-a183-69b16092f447",
   "metadata": {},
   "source": [
    "# 여기서부터 모델 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "24ce4b0a-44d7-4ea8-b04b-964182b709c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성 (미니 배치로 학습시키기 위해)\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_valid, y_valid)\n",
    "\n",
    "# DataLoader 생성 (shuffle은 학습 데이터에만 적용)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2411013f-ad48-44ba-ad52-8342e5bf6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device 설정 (CUDA GPU 또는 CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "654a3ab1-0b4a-45f4-ab84-b656deddae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 ## # 회귀 \n",
    "class LinearNN(nn.Module):\n",
    "    def __init__(self, input_dim, units_per_layer, dropout_rate):\n",
    "        super(LinearNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, units_per_layer)\n",
    "        self.fc2 = nn.Linear(units_per_layer, units_per_layer)\n",
    "        self.fc3 = nn.Linear(units_per_layer, 1)  # 출력 뉴런은 1개 (회귀 문제)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)  # 활성화 함수를 사용하지 않음 (회귀 문제)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ad2bc-f20b-4253-aa5c-f6595c5d3c48",
   "metadata": {},
   "source": [
    "# 그리드 서치 / 회귀 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0de3423-a7fc-4f81-be2f-15e62999c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정 리스트\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64, 128]\n",
    "units_per_layers = [32, 64, 128]\n",
    "dropout_rates = [0.3, 0.5]\n",
    "\n",
    "# 최적의 결과를 추적하기 위한 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "best_hyperparams = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "af227db8-3e90-44cb-be97-ce5795e2c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.001, batch_size=32, units_per_layer=32, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2995, Val Loss: 0.2641\n",
      "Epoch 2/100, Loss: 0.2853, Val Loss: 0.2340\n",
      "Epoch 3/100, Loss: 0.3071, Val Loss: 0.2085\n",
      "Epoch 4/100, Loss: 0.2234, Val Loss: 0.1855\n",
      "Epoch 5/100, Loss: 0.1967, Val Loss: 0.1643\n",
      "Epoch 6/100, Loss: 0.1702, Val Loss: 0.1445\n",
      "Epoch 7/100, Loss: 0.1718, Val Loss: 0.1370\n",
      "Epoch 8/100, Loss: 0.1495, Val Loss: 0.1404\n",
      "Epoch 9/100, Loss: 0.1506, Val Loss: 0.1425\n",
      "Epoch 10/100, Loss: 0.1272, Val Loss: 0.1440\n",
      "Epoch 11/100, Loss: 0.1111, Val Loss: 0.1455\n",
      "Epoch 12/100, Loss: 0.0950, Val Loss: 0.1495\n",
      "Epoch 13/100, Loss: 0.0951, Val Loss: 0.1514\n",
      "Epoch 14/100, Loss: 0.0860, Val Loss: 0.1510\n",
      "Epoch 15/100, Loss: 0.0674, Val Loss: 0.1488\n",
      "Epoch 16/100, Loss: 0.0594, Val Loss: 0.1452\n",
      "Epoch 17/100, Loss: 0.0602, Val Loss: 0.1418\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=32, units_per_layer=32, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.2624, Val Loss: 0.1977\n",
      "Epoch 2/100, Loss: 0.1901, Val Loss: 0.1902\n",
      "Epoch 3/100, Loss: 0.1622, Val Loss: 0.1832\n",
      "Epoch 4/100, Loss: 0.1822, Val Loss: 0.1759\n",
      "Epoch 5/100, Loss: 0.1438, Val Loss: 0.1691\n",
      "Epoch 6/100, Loss: 0.1624, Val Loss: 0.1633\n",
      "Epoch 7/100, Loss: 0.1543, Val Loss: 0.1576\n",
      "Epoch 8/100, Loss: 0.1142, Val Loss: 0.1521\n",
      "Epoch 9/100, Loss: 0.1009, Val Loss: 0.1475\n",
      "Epoch 10/100, Loss: 0.1150, Val Loss: 0.1430\n",
      "Epoch 11/100, Loss: 0.0733, Val Loss: 0.1387\n",
      "Epoch 12/100, Loss: 0.0695, Val Loss: 0.1333\n",
      "Epoch 13/100, Loss: 0.0910, Val Loss: 0.1272\n",
      "Epoch 14/100, Loss: 0.0679, Val Loss: 0.1207\n",
      "Epoch 15/100, Loss: 0.0783, Val Loss: 0.1182\n",
      "Epoch 16/100, Loss: 0.0599, Val Loss: 0.1174\n",
      "Epoch 17/100, Loss: 0.0572, Val Loss: 0.1175\n",
      "Epoch 18/100, Loss: 0.0678, Val Loss: 0.1175\n",
      "Epoch 19/100, Loss: 0.0442, Val Loss: 0.1176\n",
      "Epoch 20/100, Loss: 0.0523, Val Loss: 0.1191\n",
      "Epoch 21/100, Loss: 0.0431, Val Loss: 0.1191\n",
      "Epoch 22/100, Loss: 0.0345, Val Loss: 0.1182\n",
      "Epoch 23/100, Loss: 0.0285, Val Loss: 0.1175\n",
      "Epoch 24/100, Loss: 0.0281, Val Loss: 0.1175\n",
      "Epoch 25/100, Loss: 0.0254, Val Loss: 0.1171\n",
      "Epoch 26/100, Loss: 0.0302, Val Loss: 0.1160\n",
      "Epoch 27/100, Loss: 0.0342, Val Loss: 0.1145\n",
      "Epoch 28/100, Loss: 0.0188, Val Loss: 0.1136\n",
      "Epoch 29/100, Loss: 0.0290, Val Loss: 0.1128\n",
      "Epoch 30/100, Loss: 0.0198, Val Loss: 0.1113\n",
      "Epoch 31/100, Loss: 0.0208, Val Loss: 0.1103\n",
      "Epoch 32/100, Loss: 0.0269, Val Loss: 0.1100\n",
      "Epoch 33/100, Loss: 0.0173, Val Loss: 0.1101\n",
      "Epoch 34/100, Loss: 0.0159, Val Loss: 0.1116\n",
      "Epoch 35/100, Loss: 0.0161, Val Loss: 0.1135\n",
      "Epoch 36/100, Loss: 0.0236, Val Loss: 0.1136\n",
      "Epoch 37/100, Loss: 0.0226, Val Loss: 0.1110\n",
      "Epoch 38/100, Loss: 0.0139, Val Loss: 0.1083\n",
      "Epoch 39/100, Loss: 0.0181, Val Loss: 0.1073\n",
      "Epoch 40/100, Loss: 0.0193, Val Loss: 0.1082\n",
      "Epoch 41/100, Loss: 0.0112, Val Loss: 0.1121\n",
      "Epoch 42/100, Loss: 0.0104, Val Loss: 0.1149\n",
      "Epoch 43/100, Loss: 0.0109, Val Loss: 0.1157\n",
      "Epoch 44/100, Loss: 0.0114, Val Loss: 0.1144\n",
      "Epoch 45/100, Loss: 0.0114, Val Loss: 0.1125\n",
      "Epoch 46/100, Loss: 0.0077, Val Loss: 0.1123\n",
      "Epoch 47/100, Loss: 0.0080, Val Loss: 0.1138\n",
      "Epoch 48/100, Loss: 0.0120, Val Loss: 0.1160\n",
      "Epoch 49/100, Loss: 0.0109, Val Loss: 0.1181\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=32, units_per_layer=64, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2802, Val Loss: 0.1903\n",
      "Epoch 2/100, Loss: 0.2174, Val Loss: 0.1644\n",
      "Epoch 3/100, Loss: 0.1642, Val Loss: 0.1542\n",
      "Epoch 4/100, Loss: 0.1667, Val Loss: 0.1514\n",
      "Epoch 5/100, Loss: 0.1224, Val Loss: 0.1479\n",
      "Epoch 6/100, Loss: 0.1006, Val Loss: 0.1448\n",
      "Epoch 7/100, Loss: 0.1098, Val Loss: 0.1409\n",
      "Epoch 8/100, Loss: 0.0821, Val Loss: 0.1382\n",
      "Epoch 9/100, Loss: 0.1002, Val Loss: 0.1358\n",
      "Epoch 10/100, Loss: 0.0670, Val Loss: 0.1342\n",
      "Epoch 11/100, Loss: 0.0544, Val Loss: 0.1333\n",
      "Epoch 12/100, Loss: 0.0589, Val Loss: 0.1330\n",
      "Epoch 13/100, Loss: 0.0626, Val Loss: 0.1309\n",
      "Epoch 14/100, Loss: 0.0425, Val Loss: 0.1287\n",
      "Epoch 15/100, Loss: 0.0408, Val Loss: 0.1270\n",
      "Epoch 16/100, Loss: 0.0307, Val Loss: 0.1277\n",
      "Epoch 17/100, Loss: 0.0270, Val Loss: 0.1306\n",
      "Epoch 18/100, Loss: 0.0250, Val Loss: 0.1337\n",
      "Epoch 19/100, Loss: 0.0250, Val Loss: 0.1334\n",
      "Epoch 20/100, Loss: 0.0279, Val Loss: 0.1310\n",
      "Epoch 21/100, Loss: 0.0284, Val Loss: 0.1287\n",
      "Epoch 22/100, Loss: 0.0259, Val Loss: 0.1279\n",
      "Epoch 23/100, Loss: 0.0235, Val Loss: 0.1273\n",
      "Epoch 24/100, Loss: 0.0240, Val Loss: 0.1262\n",
      "Epoch 25/100, Loss: 0.0317, Val Loss: 0.1254\n",
      "Epoch 26/100, Loss: 0.0222, Val Loss: 0.1263\n",
      "Epoch 27/100, Loss: 0.0189, Val Loss: 0.1291\n",
      "Epoch 28/100, Loss: 0.0155, Val Loss: 0.1321\n",
      "Epoch 29/100, Loss: 0.0164, Val Loss: 0.1319\n",
      "Epoch 30/100, Loss: 0.0155, Val Loss: 0.1336\n",
      "Epoch 31/100, Loss: 0.0229, Val Loss: 0.1353\n",
      "Epoch 32/100, Loss: 0.0178, Val Loss: 0.1331\n",
      "Epoch 33/100, Loss: 0.0172, Val Loss: 0.1326\n",
      "Epoch 34/100, Loss: 0.0159, Val Loss: 0.1336\n",
      "Epoch 35/100, Loss: 0.0149, Val Loss: 0.1341\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=32, units_per_layer=64, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.2505, Val Loss: 0.2015\n",
      "Epoch 2/100, Loss: 0.1772, Val Loss: 0.1771\n",
      "Epoch 3/100, Loss: 0.1697, Val Loss: 0.1638\n",
      "Epoch 4/100, Loss: 0.1604, Val Loss: 0.1602\n",
      "Epoch 5/100, Loss: 0.1198, Val Loss: 0.1610\n",
      "Epoch 6/100, Loss: 0.0990, Val Loss: 0.1595\n",
      "Epoch 7/100, Loss: 0.0950, Val Loss: 0.1568\n",
      "Epoch 8/100, Loss: 0.0853, Val Loss: 0.1556\n",
      "Epoch 9/100, Loss: 0.0694, Val Loss: 0.1533\n",
      "Epoch 10/100, Loss: 0.0684, Val Loss: 0.1505\n",
      "Epoch 11/100, Loss: 0.0680, Val Loss: 0.1480\n",
      "Epoch 12/100, Loss: 0.0695, Val Loss: 0.1449\n",
      "Epoch 13/100, Loss: 0.0535, Val Loss: 0.1478\n",
      "Epoch 14/100, Loss: 0.0425, Val Loss: 0.1516\n",
      "Epoch 15/100, Loss: 0.0342, Val Loss: 0.1503\n",
      "Epoch 16/100, Loss: 0.0384, Val Loss: 0.1476\n",
      "Epoch 17/100, Loss: 0.0331, Val Loss: 0.1479\n",
      "Epoch 18/100, Loss: 0.0388, Val Loss: 0.1513\n",
      "Epoch 19/100, Loss: 0.0368, Val Loss: 0.1542\n",
      "Epoch 20/100, Loss: 0.0244, Val Loss: 0.1560\n",
      "Epoch 21/100, Loss: 0.0214, Val Loss: 0.1556\n",
      "Epoch 22/100, Loss: 0.0182, Val Loss: 0.1549\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=32, units_per_layer=128, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2556, Val Loss: 0.1973\n",
      "Epoch 2/100, Loss: 0.2119, Val Loss: 0.1680\n",
      "Epoch 3/100, Loss: 0.1673, Val Loss: 0.1792\n",
      "Epoch 4/100, Loss: 0.1130, Val Loss: 0.1788\n",
      "Epoch 5/100, Loss: 0.1218, Val Loss: 0.1641\n",
      "Epoch 6/100, Loss: 0.1182, Val Loss: 0.1522\n",
      "Epoch 7/100, Loss: 0.0822, Val Loss: 0.1481\n",
      "Epoch 8/100, Loss: 0.0712, Val Loss: 0.1478\n",
      "Epoch 9/100, Loss: 0.0745, Val Loss: 0.1512\n",
      "Epoch 10/100, Loss: 0.0543, Val Loss: 0.1517\n",
      "Epoch 11/100, Loss: 0.0502, Val Loss: 0.1480\n",
      "Epoch 12/100, Loss: 0.0580, Val Loss: 0.1415\n",
      "Epoch 13/100, Loss: 0.0357, Val Loss: 0.1379\n",
      "Epoch 14/100, Loss: 0.0479, Val Loss: 0.1370\n",
      "Epoch 15/100, Loss: 0.0483, Val Loss: 0.1361\n",
      "Epoch 16/100, Loss: 0.0462, Val Loss: 0.1367\n",
      "Epoch 17/100, Loss: 0.0345, Val Loss: 0.1372\n",
      "Epoch 18/100, Loss: 0.0302, Val Loss: 0.1353\n",
      "Epoch 19/100, Loss: 0.0337, Val Loss: 0.1308\n",
      "Epoch 20/100, Loss: 0.0256, Val Loss: 0.1276\n",
      "Epoch 21/100, Loss: 0.0254, Val Loss: 0.1273\n",
      "Epoch 22/100, Loss: 0.0205, Val Loss: 0.1299\n",
      "Epoch 23/100, Loss: 0.0237, Val Loss: 0.1308\n",
      "Epoch 24/100, Loss: 0.0238, Val Loss: 0.1263\n",
      "Epoch 25/100, Loss: 0.0207, Val Loss: 0.1252\n",
      "Epoch 26/100, Loss: 0.0259, Val Loss: 0.1245\n",
      "Epoch 27/100, Loss: 0.0215, Val Loss: 0.1238\n",
      "Epoch 28/100, Loss: 0.0199, Val Loss: 0.1257\n",
      "Epoch 29/100, Loss: 0.0229, Val Loss: 0.1275\n",
      "Epoch 30/100, Loss: 0.0307, Val Loss: 0.1246\n",
      "Epoch 31/100, Loss: 0.0184, Val Loss: 0.1190\n",
      "Epoch 32/100, Loss: 0.0238, Val Loss: 0.1171\n",
      "Epoch 33/100, Loss: 0.0186, Val Loss: 0.1202\n",
      "Epoch 34/100, Loss: 0.0214, Val Loss: 0.1262\n",
      "Epoch 35/100, Loss: 0.0160, Val Loss: 0.1310\n",
      "Epoch 36/100, Loss: 0.0202, Val Loss: 0.1253\n",
      "Epoch 37/100, Loss: 0.0191, Val Loss: 0.1227\n",
      "Epoch 38/100, Loss: 0.0231, Val Loss: 0.1247\n",
      "Epoch 39/100, Loss: 0.0167, Val Loss: 0.1262\n",
      "Epoch 40/100, Loss: 0.0127, Val Loss: 0.1235\n",
      "Epoch 41/100, Loss: 0.0134, Val Loss: 0.1248\n",
      "Epoch 42/100, Loss: 0.0183, Val Loss: 0.1284\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=32, units_per_layer=128, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.3389, Val Loss: 0.1795\n",
      "Epoch 2/100, Loss: 0.1821, Val Loss: 0.1504\n",
      "Epoch 3/100, Loss: 0.1713, Val Loss: 0.1423\n",
      "Epoch 4/100, Loss: 0.1130, Val Loss: 0.1366\n",
      "Epoch 5/100, Loss: 0.1227, Val Loss: 0.1358\n",
      "Epoch 6/100, Loss: 0.0775, Val Loss: 0.1416\n",
      "Epoch 7/100, Loss: 0.0665, Val Loss: 0.1375\n",
      "Epoch 8/100, Loss: 0.0501, Val Loss: 0.1362\n",
      "Epoch 9/100, Loss: 0.0600, Val Loss: 0.1368\n",
      "Epoch 10/100, Loss: 0.0609, Val Loss: 0.1262\n",
      "Epoch 11/100, Loss: 0.0503, Val Loss: 0.1197\n",
      "Epoch 12/100, Loss: 0.0569, Val Loss: 0.1209\n",
      "Epoch 13/100, Loss: 0.0424, Val Loss: 0.1191\n",
      "Epoch 14/100, Loss: 0.0331, Val Loss: 0.1214\n",
      "Epoch 15/100, Loss: 0.0311, Val Loss: 0.1269\n",
      "Epoch 16/100, Loss: 0.0343, Val Loss: 0.1310\n",
      "Epoch 17/100, Loss: 0.0386, Val Loss: 0.1336\n",
      "Epoch 18/100, Loss: 0.0403, Val Loss: 0.1354\n",
      "Epoch 19/100, Loss: 0.0400, Val Loss: 0.1350\n",
      "Epoch 20/100, Loss: 0.0387, Val Loss: 0.1295\n",
      "Epoch 21/100, Loss: 0.0268, Val Loss: 0.1229\n",
      "Epoch 22/100, Loss: 0.0258, Val Loss: 0.1199\n",
      "Epoch 23/100, Loss: 0.0285, Val Loss: 0.1205\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=64, units_per_layer=32, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2748, Val Loss: 0.2369\n",
      "Epoch 2/100, Loss: 0.2592, Val Loss: 0.2209\n",
      "Epoch 3/100, Loss: 0.2407, Val Loss: 0.2078\n",
      "Epoch 4/100, Loss: 0.2231, Val Loss: 0.1962\n",
      "Epoch 5/100, Loss: 0.2081, Val Loss: 0.1859\n",
      "Epoch 6/100, Loss: 0.1929, Val Loss: 0.1781\n",
      "Epoch 7/100, Loss: 0.1792, Val Loss: 0.1713\n",
      "Epoch 8/100, Loss: 0.1686, Val Loss: 0.1651\n",
      "Epoch 9/100, Loss: 0.1592, Val Loss: 0.1605\n",
      "Epoch 10/100, Loss: 0.1524, Val Loss: 0.1567\n",
      "Epoch 11/100, Loss: 0.1454, Val Loss: 0.1531\n",
      "Epoch 12/100, Loss: 0.1393, Val Loss: 0.1498\n",
      "Epoch 13/100, Loss: 0.1339, Val Loss: 0.1465\n",
      "Epoch 14/100, Loss: 0.1287, Val Loss: 0.1435\n",
      "Epoch 15/100, Loss: 0.1228, Val Loss: 0.1404\n",
      "Epoch 16/100, Loss: 0.1161, Val Loss: 0.1382\n",
      "Epoch 17/100, Loss: 0.1089, Val Loss: 0.1362\n",
      "Epoch 18/100, Loss: 0.1006, Val Loss: 0.1339\n",
      "Epoch 19/100, Loss: 0.0920, Val Loss: 0.1313\n",
      "Epoch 20/100, Loss: 0.0837, Val Loss: 0.1286\n",
      "Epoch 21/100, Loss: 0.0785, Val Loss: 0.1259\n",
      "Epoch 22/100, Loss: 0.0753, Val Loss: 0.1234\n",
      "Epoch 23/100, Loss: 0.0727, Val Loss: 0.1212\n",
      "Epoch 24/100, Loss: 0.0699, Val Loss: 0.1192\n",
      "Epoch 25/100, Loss: 0.0673, Val Loss: 0.1175\n",
      "Epoch 26/100, Loss: 0.0639, Val Loss: 0.1169\n",
      "Epoch 27/100, Loss: 0.0597, Val Loss: 0.1178\n",
      "Epoch 28/100, Loss: 0.0545, Val Loss: 0.1199\n",
      "Epoch 29/100, Loss: 0.0497, Val Loss: 0.1222\n",
      "Epoch 30/100, Loss: 0.0455, Val Loss: 0.1249\n",
      "Epoch 31/100, Loss: 0.0424, Val Loss: 0.1263\n",
      "Epoch 32/100, Loss: 0.0388, Val Loss: 0.1265\n",
      "Epoch 33/100, Loss: 0.0349, Val Loss: 0.1261\n",
      "Epoch 34/100, Loss: 0.0309, Val Loss: 0.1257\n",
      "Epoch 35/100, Loss: 0.0283, Val Loss: 0.1259\n",
      "Epoch 36/100, Loss: 0.0277, Val Loss: 0.1266\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=64, units_per_layer=32, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.5180, Val Loss: 0.4583\n",
      "Epoch 2/100, Loss: 0.4733, Val Loss: 0.4384\n",
      "Epoch 3/100, Loss: 0.4499, Val Loss: 0.4185\n",
      "Epoch 4/100, Loss: 0.4263, Val Loss: 0.3985\n",
      "Epoch 5/100, Loss: 0.4027, Val Loss: 0.3786\n",
      "Epoch 6/100, Loss: 0.3790, Val Loss: 0.3606\n",
      "Epoch 7/100, Loss: 0.3552, Val Loss: 0.3427\n",
      "Epoch 8/100, Loss: 0.3313, Val Loss: 0.3245\n",
      "Epoch 9/100, Loss: 0.3110, Val Loss: 0.3064\n",
      "Epoch 10/100, Loss: 0.2916, Val Loss: 0.2903\n",
      "Epoch 11/100, Loss: 0.2721, Val Loss: 0.2763\n",
      "Epoch 12/100, Loss: 0.2531, Val Loss: 0.2637\n",
      "Epoch 13/100, Loss: 0.2342, Val Loss: 0.2525\n",
      "Epoch 14/100, Loss: 0.2163, Val Loss: 0.2435\n",
      "Epoch 15/100, Loss: 0.1995, Val Loss: 0.2362\n",
      "Epoch 16/100, Loss: 0.1843, Val Loss: 0.2310\n",
      "Epoch 17/100, Loss: 0.1698, Val Loss: 0.2271\n",
      "Epoch 18/100, Loss: 0.1566, Val Loss: 0.2229\n",
      "Epoch 19/100, Loss: 0.1469, Val Loss: 0.2184\n",
      "Epoch 20/100, Loss: 0.1402, Val Loss: 0.2142\n",
      "Epoch 21/100, Loss: 0.1350, Val Loss: 0.2113\n",
      "Epoch 22/100, Loss: 0.1322, Val Loss: 0.2100\n",
      "Epoch 23/100, Loss: 0.1292, Val Loss: 0.2093\n",
      "Epoch 24/100, Loss: 0.1269, Val Loss: 0.2088\n",
      "Epoch 25/100, Loss: 0.1234, Val Loss: 0.2084\n",
      "Epoch 26/100, Loss: 0.1195, Val Loss: 0.2077\n",
      "Epoch 27/100, Loss: 0.1141, Val Loss: 0.2070\n",
      "Epoch 28/100, Loss: 0.1079, Val Loss: 0.2063\n",
      "Epoch 29/100, Loss: 0.1005, Val Loss: 0.2057\n",
      "Epoch 30/100, Loss: 0.0931, Val Loss: 0.2050\n",
      "Epoch 31/100, Loss: 0.0857, Val Loss: 0.2042\n",
      "Epoch 32/100, Loss: 0.0812, Val Loss: 0.2029\n",
      "Epoch 33/100, Loss: 0.0788, Val Loss: 0.2009\n",
      "Epoch 34/100, Loss: 0.0758, Val Loss: 0.1980\n",
      "Epoch 35/100, Loss: 0.0743, Val Loss: 0.1947\n",
      "Epoch 36/100, Loss: 0.0718, Val Loss: 0.1909\n",
      "Epoch 37/100, Loss: 0.0688, Val Loss: 0.1870\n",
      "Epoch 38/100, Loss: 0.0648, Val Loss: 0.1832\n",
      "Epoch 39/100, Loss: 0.0607, Val Loss: 0.1799\n",
      "Epoch 40/100, Loss: 0.0570, Val Loss: 0.1772\n",
      "Epoch 41/100, Loss: 0.0557, Val Loss: 0.1756\n",
      "Epoch 42/100, Loss: 0.0548, Val Loss: 0.1747\n",
      "Epoch 43/100, Loss: 0.0550, Val Loss: 0.1743\n",
      "Epoch 44/100, Loss: 0.0537, Val Loss: 0.1743\n",
      "Epoch 45/100, Loss: 0.0507, Val Loss: 0.1749\n",
      "Epoch 46/100, Loss: 0.0470, Val Loss: 0.1756\n",
      "Epoch 47/100, Loss: 0.0455, Val Loss: 0.1758\n",
      "Epoch 48/100, Loss: 0.0445, Val Loss: 0.1756\n",
      "Epoch 49/100, Loss: 0.0437, Val Loss: 0.1752\n",
      "Epoch 50/100, Loss: 0.0417, Val Loss: 0.1742\n",
      "Epoch 51/100, Loss: 0.0388, Val Loss: 0.1727\n",
      "Epoch 52/100, Loss: 0.0366, Val Loss: 0.1713\n",
      "Epoch 53/100, Loss: 0.0348, Val Loss: 0.1705\n",
      "Epoch 54/100, Loss: 0.0336, Val Loss: 0.1700\n",
      "Epoch 55/100, Loss: 0.0319, Val Loss: 0.1696\n",
      "Epoch 56/100, Loss: 0.0308, Val Loss: 0.1688\n",
      "Epoch 57/100, Loss: 0.0296, Val Loss: 0.1679\n",
      "Epoch 58/100, Loss: 0.0277, Val Loss: 0.1671\n",
      "Epoch 59/100, Loss: 0.0266, Val Loss: 0.1668\n",
      "Epoch 60/100, Loss: 0.0260, Val Loss: 0.1668\n",
      "Epoch 61/100, Loss: 0.0248, Val Loss: 0.1671\n",
      "Epoch 62/100, Loss: 0.0238, Val Loss: 0.1674\n",
      "Epoch 63/100, Loss: 0.0227, Val Loss: 0.1669\n",
      "Epoch 64/100, Loss: 0.0212, Val Loss: 0.1656\n",
      "Epoch 65/100, Loss: 0.0196, Val Loss: 0.1644\n",
      "Epoch 66/100, Loss: 0.0181, Val Loss: 0.1635\n",
      "Epoch 67/100, Loss: 0.0164, Val Loss: 0.1634\n",
      "Epoch 68/100, Loss: 0.0160, Val Loss: 0.1641\n",
      "Epoch 69/100, Loss: 0.0152, Val Loss: 0.1652\n",
      "Epoch 70/100, Loss: 0.0147, Val Loss: 0.1658\n",
      "Epoch 71/100, Loss: 0.0142, Val Loss: 0.1657\n",
      "Epoch 72/100, Loss: 0.0133, Val Loss: 0.1652\n",
      "Epoch 73/100, Loss: 0.0129, Val Loss: 0.1645\n",
      "Epoch 74/100, Loss: 0.0116, Val Loss: 0.1632\n",
      "Epoch 75/100, Loss: 0.0113, Val Loss: 0.1630\n",
      "Epoch 76/100, Loss: 0.0102, Val Loss: 0.1636\n",
      "Epoch 77/100, Loss: 0.0087, Val Loss: 0.1643\n",
      "Epoch 78/100, Loss: 0.0079, Val Loss: 0.1645\n",
      "Epoch 79/100, Loss: 0.0069, Val Loss: 0.1641\n",
      "Epoch 80/100, Loss: 0.0063, Val Loss: 0.1636\n",
      "Epoch 81/100, Loss: 0.0053, Val Loss: 0.1644\n",
      "Epoch 82/100, Loss: 0.0044, Val Loss: 0.1655\n",
      "Epoch 83/100, Loss: 0.0045, Val Loss: 0.1661\n",
      "Epoch 84/100, Loss: 0.0046, Val Loss: 0.1656\n",
      "Epoch 85/100, Loss: 0.0043, Val Loss: 0.1645\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=64, units_per_layer=64, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2533, Val Loss: 0.2204\n",
      "Epoch 2/100, Loss: 0.2454, Val Loss: 0.1945\n",
      "Epoch 3/100, Loss: 0.2157, Val Loss: 0.1731\n",
      "Epoch 4/100, Loss: 0.1902, Val Loss: 0.1596\n",
      "Epoch 5/100, Loss: 0.1723, Val Loss: 0.1568\n",
      "Epoch 6/100, Loss: 0.1564, Val Loss: 0.1559\n",
      "Epoch 7/100, Loss: 0.1401, Val Loss: 0.1563\n",
      "Epoch 8/100, Loss: 0.1305, Val Loss: 0.1572\n",
      "Epoch 9/100, Loss: 0.1231, Val Loss: 0.1574\n",
      "Epoch 10/100, Loss: 0.1159, Val Loss: 0.1569\n",
      "Epoch 11/100, Loss: 0.1079, Val Loss: 0.1549\n",
      "Epoch 12/100, Loss: 0.0979, Val Loss: 0.1519\n",
      "Epoch 13/100, Loss: 0.0865, Val Loss: 0.1491\n",
      "Epoch 14/100, Loss: 0.0756, Val Loss: 0.1463\n",
      "Epoch 15/100, Loss: 0.0680, Val Loss: 0.1440\n",
      "Epoch 16/100, Loss: 0.0624, Val Loss: 0.1419\n",
      "Epoch 17/100, Loss: 0.0586, Val Loss: 0.1403\n",
      "Epoch 18/100, Loss: 0.0532, Val Loss: 0.1394\n",
      "Epoch 19/100, Loss: 0.0451, Val Loss: 0.1392\n",
      "Epoch 20/100, Loss: 0.0401, Val Loss: 0.1392\n",
      "Epoch 21/100, Loss: 0.0388, Val Loss: 0.1389\n",
      "Epoch 22/100, Loss: 0.0378, Val Loss: 0.1381\n",
      "Epoch 23/100, Loss: 0.0364, Val Loss: 0.1368\n",
      "Epoch 24/100, Loss: 0.0334, Val Loss: 0.1353\n",
      "Epoch 25/100, Loss: 0.0308, Val Loss: 0.1334\n",
      "Epoch 26/100, Loss: 0.0300, Val Loss: 0.1320\n",
      "Epoch 27/100, Loss: 0.0293, Val Loss: 0.1310\n",
      "Epoch 28/100, Loss: 0.0284, Val Loss: 0.1306\n",
      "Epoch 29/100, Loss: 0.0267, Val Loss: 0.1311\n",
      "Epoch 30/100, Loss: 0.0239, Val Loss: 0.1317\n",
      "Epoch 31/100, Loss: 0.0228, Val Loss: 0.1317\n",
      "Epoch 32/100, Loss: 0.0225, Val Loss: 0.1309\n",
      "Epoch 33/100, Loss: 0.0208, Val Loss: 0.1300\n",
      "Epoch 34/100, Loss: 0.0187, Val Loss: 0.1294\n",
      "Epoch 35/100, Loss: 0.0185, Val Loss: 0.1297\n",
      "Epoch 36/100, Loss: 0.0169, Val Loss: 0.1310\n",
      "Epoch 37/100, Loss: 0.0140, Val Loss: 0.1320\n",
      "Epoch 38/100, Loss: 0.0131, Val Loss: 0.1323\n",
      "Epoch 39/100, Loss: 0.0120, Val Loss: 0.1317\n",
      "Epoch 40/100, Loss: 0.0117, Val Loss: 0.1303\n",
      "Epoch 41/100, Loss: 0.0111, Val Loss: 0.1281\n",
      "Epoch 42/100, Loss: 0.0103, Val Loss: 0.1264\n",
      "Epoch 43/100, Loss: 0.0100, Val Loss: 0.1255\n",
      "Epoch 44/100, Loss: 0.0091, Val Loss: 0.1254\n",
      "Epoch 45/100, Loss: 0.0090, Val Loss: 0.1251\n",
      "Epoch 46/100, Loss: 0.0087, Val Loss: 0.1246\n",
      "Epoch 47/100, Loss: 0.0085, Val Loss: 0.1241\n",
      "Epoch 48/100, Loss: 0.0083, Val Loss: 0.1245\n",
      "Epoch 49/100, Loss: 0.0075, Val Loss: 0.1249\n",
      "Epoch 50/100, Loss: 0.0065, Val Loss: 0.1250\n",
      "Epoch 51/100, Loss: 0.0059, Val Loss: 0.1250\n",
      "Epoch 52/100, Loss: 0.0059, Val Loss: 0.1255\n",
      "Epoch 53/100, Loss: 0.0056, Val Loss: 0.1255\n",
      "Epoch 54/100, Loss: 0.0048, Val Loss: 0.1254\n",
      "Epoch 55/100, Loss: 0.0046, Val Loss: 0.1253\n",
      "Epoch 56/100, Loss: 0.0046, Val Loss: 0.1254\n",
      "Epoch 57/100, Loss: 0.0054, Val Loss: 0.1254\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=64, units_per_layer=64, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.3698, Val Loss: 0.2835\n",
      "Epoch 2/100, Loss: 0.3151, Val Loss: 0.2519\n",
      "Epoch 3/100, Loss: 0.2848, Val Loss: 0.2231\n",
      "Epoch 4/100, Loss: 0.2577, Val Loss: 0.1997\n",
      "Epoch 5/100, Loss: 0.2361, Val Loss: 0.1807\n",
      "Epoch 6/100, Loss: 0.2147, Val Loss: 0.1699\n",
      "Epoch 7/100, Loss: 0.1963, Val Loss: 0.1653\n",
      "Epoch 8/100, Loss: 0.1811, Val Loss: 0.1635\n",
      "Epoch 9/100, Loss: 0.1690, Val Loss: 0.1631\n",
      "Epoch 10/100, Loss: 0.1573, Val Loss: 0.1622\n",
      "Epoch 11/100, Loss: 0.1454, Val Loss: 0.1619\n",
      "Epoch 12/100, Loss: 0.1350, Val Loss: 0.1611\n",
      "Epoch 13/100, Loss: 0.1268, Val Loss: 0.1604\n",
      "Epoch 14/100, Loss: 0.1187, Val Loss: 0.1590\n",
      "Epoch 15/100, Loss: 0.1121, Val Loss: 0.1572\n",
      "Epoch 16/100, Loss: 0.1038, Val Loss: 0.1541\n",
      "Epoch 17/100, Loss: 0.0946, Val Loss: 0.1503\n",
      "Epoch 18/100, Loss: 0.0829, Val Loss: 0.1462\n",
      "Epoch 19/100, Loss: 0.0722, Val Loss: 0.1420\n",
      "Epoch 20/100, Loss: 0.0637, Val Loss: 0.1381\n",
      "Epoch 21/100, Loss: 0.0600, Val Loss: 0.1356\n",
      "Epoch 22/100, Loss: 0.0615, Val Loss: 0.1351\n",
      "Epoch 23/100, Loss: 0.0623, Val Loss: 0.1350\n",
      "Epoch 24/100, Loss: 0.0612, Val Loss: 0.1355\n",
      "Epoch 25/100, Loss: 0.0566, Val Loss: 0.1371\n",
      "Epoch 26/100, Loss: 0.0519, Val Loss: 0.1390\n",
      "Epoch 27/100, Loss: 0.0482, Val Loss: 0.1403\n",
      "Epoch 28/100, Loss: 0.0460, Val Loss: 0.1410\n",
      "Epoch 29/100, Loss: 0.0429, Val Loss: 0.1404\n",
      "Epoch 30/100, Loss: 0.0398, Val Loss: 0.1392\n",
      "Epoch 31/100, Loss: 0.0363, Val Loss: 0.1371\n",
      "Epoch 32/100, Loss: 0.0328, Val Loss: 0.1349\n",
      "Epoch 33/100, Loss: 0.0315, Val Loss: 0.1332\n",
      "Epoch 34/100, Loss: 0.0313, Val Loss: 0.1323\n",
      "Epoch 35/100, Loss: 0.0299, Val Loss: 0.1323\n",
      "Epoch 36/100, Loss: 0.0280, Val Loss: 0.1323\n",
      "Epoch 37/100, Loss: 0.0266, Val Loss: 0.1329\n",
      "Epoch 38/100, Loss: 0.0254, Val Loss: 0.1330\n",
      "Epoch 39/100, Loss: 0.0243, Val Loss: 0.1327\n",
      "Epoch 40/100, Loss: 0.0230, Val Loss: 0.1322\n",
      "Epoch 41/100, Loss: 0.0223, Val Loss: 0.1319\n",
      "Epoch 42/100, Loss: 0.0204, Val Loss: 0.1317\n",
      "Epoch 43/100, Loss: 0.0190, Val Loss: 0.1321\n",
      "Epoch 44/100, Loss: 0.0183, Val Loss: 0.1328\n",
      "Epoch 45/100, Loss: 0.0169, Val Loss: 0.1331\n",
      "Epoch 46/100, Loss: 0.0154, Val Loss: 0.1338\n",
      "Epoch 47/100, Loss: 0.0151, Val Loss: 0.1345\n",
      "Epoch 48/100, Loss: 0.0146, Val Loss: 0.1343\n",
      "Epoch 49/100, Loss: 0.0142, Val Loss: 0.1330\n",
      "Epoch 50/100, Loss: 0.0135, Val Loss: 0.1318\n",
      "Epoch 51/100, Loss: 0.0128, Val Loss: 0.1318\n",
      "Epoch 52/100, Loss: 0.0121, Val Loss: 0.1326\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=64, units_per_layer=128, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2769, Val Loss: 0.1807\n",
      "Epoch 2/100, Loss: 0.1941, Val Loss: 0.1451\n",
      "Epoch 3/100, Loss: 0.1622, Val Loss: 0.1346\n",
      "Epoch 4/100, Loss: 0.1477, Val Loss: 0.1354\n",
      "Epoch 5/100, Loss: 0.1338, Val Loss: 0.1365\n",
      "Epoch 6/100, Loss: 0.1139, Val Loss: 0.1384\n",
      "Epoch 7/100, Loss: 0.0939, Val Loss: 0.1324\n",
      "Epoch 8/100, Loss: 0.0756, Val Loss: 0.1250\n",
      "Epoch 9/100, Loss: 0.0608, Val Loss: 0.1199\n",
      "Epoch 10/100, Loss: 0.0559, Val Loss: 0.1196\n",
      "Epoch 11/100, Loss: 0.0559, Val Loss: 0.1215\n",
      "Epoch 12/100, Loss: 0.0520, Val Loss: 0.1242\n",
      "Epoch 13/100, Loss: 0.0457, Val Loss: 0.1280\n",
      "Epoch 14/100, Loss: 0.0405, Val Loss: 0.1303\n",
      "Epoch 15/100, Loss: 0.0387, Val Loss: 0.1299\n",
      "Epoch 16/100, Loss: 0.0351, Val Loss: 0.1275\n",
      "Epoch 17/100, Loss: 0.0314, Val Loss: 0.1242\n",
      "Epoch 18/100, Loss: 0.0289, Val Loss: 0.1218\n",
      "Epoch 19/100, Loss: 0.0285, Val Loss: 0.1209\n",
      "Epoch 20/100, Loss: 0.0250, Val Loss: 0.1212\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=64, units_per_layer=128, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.2941, Val Loss: 0.1980\n",
      "Epoch 2/100, Loss: 0.2480, Val Loss: 0.1696\n",
      "Epoch 3/100, Loss: 0.2052, Val Loss: 0.1718\n",
      "Epoch 4/100, Loss: 0.1793, Val Loss: 0.1867\n",
      "Epoch 5/100, Loss: 0.1600, Val Loss: 0.2060\n",
      "Epoch 6/100, Loss: 0.1466, Val Loss: 0.2128\n",
      "Epoch 7/100, Loss: 0.1364, Val Loss: 0.2077\n",
      "Epoch 8/100, Loss: 0.1179, Val Loss: 0.1923\n",
      "Epoch 9/100, Loss: 0.0876, Val Loss: 0.1715\n",
      "Epoch 10/100, Loss: 0.0673, Val Loss: 0.1597\n",
      "Epoch 11/100, Loss: 0.0673, Val Loss: 0.1543\n",
      "Epoch 12/100, Loss: 0.0647, Val Loss: 0.1533\n",
      "Epoch 13/100, Loss: 0.0600, Val Loss: 0.1568\n",
      "Epoch 14/100, Loss: 0.0540, Val Loss: 0.1618\n",
      "Epoch 15/100, Loss: 0.0533, Val Loss: 0.1655\n",
      "Epoch 16/100, Loss: 0.0527, Val Loss: 0.1652\n",
      "Epoch 17/100, Loss: 0.0501, Val Loss: 0.1616\n",
      "Epoch 18/100, Loss: 0.0434, Val Loss: 0.1567\n",
      "Epoch 19/100, Loss: 0.0373, Val Loss: 0.1525\n",
      "Epoch 20/100, Loss: 0.0354, Val Loss: 0.1511\n",
      "Epoch 21/100, Loss: 0.0332, Val Loss: 0.1531\n",
      "Epoch 22/100, Loss: 0.0273, Val Loss: 0.1559\n",
      "Epoch 23/100, Loss: 0.0297, Val Loss: 0.1552\n",
      "Epoch 24/100, Loss: 0.0291, Val Loss: 0.1516\n",
      "Epoch 25/100, Loss: 0.0280, Val Loss: 0.1486\n",
      "Epoch 26/100, Loss: 0.0270, Val Loss: 0.1482\n",
      "Epoch 27/100, Loss: 0.0242, Val Loss: 0.1500\n",
      "Epoch 28/100, Loss: 0.0215, Val Loss: 0.1520\n",
      "Epoch 29/100, Loss: 0.0195, Val Loss: 0.1521\n",
      "Epoch 30/100, Loss: 0.0177, Val Loss: 0.1501\n",
      "Epoch 31/100, Loss: 0.0152, Val Loss: 0.1486\n",
      "Epoch 32/100, Loss: 0.0124, Val Loss: 0.1478\n",
      "Epoch 33/100, Loss: 0.0109, Val Loss: 0.1484\n",
      "Epoch 34/100, Loss: 0.0108, Val Loss: 0.1469\n",
      "Epoch 35/100, Loss: 0.0103, Val Loss: 0.1437\n",
      "Epoch 36/100, Loss: 0.0112, Val Loss: 0.1438\n",
      "Epoch 37/100, Loss: 0.0104, Val Loss: 0.1460\n",
      "Epoch 38/100, Loss: 0.0086, Val Loss: 0.1475\n",
      "Epoch 39/100, Loss: 0.0095, Val Loss: 0.1467\n",
      "Epoch 40/100, Loss: 0.0103, Val Loss: 0.1459\n",
      "Epoch 41/100, Loss: 0.0097, Val Loss: 0.1442\n",
      "Epoch 42/100, Loss: 0.0101, Val Loss: 0.1449\n",
      "Epoch 43/100, Loss: 0.0103, Val Loss: 0.1442\n",
      "Epoch 44/100, Loss: 0.0091, Val Loss: 0.1439\n",
      "Epoch 45/100, Loss: 0.0077, Val Loss: 0.1437\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=128, units_per_layer=32, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2655, Val Loss: 0.2307\n",
      "Epoch 2/100, Loss: 0.2259, Val Loss: 0.2205\n",
      "Epoch 3/100, Loss: 0.2119, Val Loss: 0.2116\n",
      "Epoch 4/100, Loss: 0.1976, Val Loss: 0.2024\n",
      "Epoch 5/100, Loss: 0.1835, Val Loss: 0.1940\n",
      "Epoch 6/100, Loss: 0.1717, Val Loss: 0.1875\n",
      "Epoch 7/100, Loss: 0.1622, Val Loss: 0.1816\n",
      "Epoch 8/100, Loss: 0.1530, Val Loss: 0.1759\n",
      "Epoch 9/100, Loss: 0.1451, Val Loss: 0.1705\n",
      "Epoch 10/100, Loss: 0.1408, Val Loss: 0.1656\n",
      "Epoch 11/100, Loss: 0.1360, Val Loss: 0.1619\n",
      "Epoch 12/100, Loss: 0.1317, Val Loss: 0.1587\n",
      "Epoch 13/100, Loss: 0.1265, Val Loss: 0.1559\n",
      "Epoch 14/100, Loss: 0.1216, Val Loss: 0.1536\n",
      "Epoch 15/100, Loss: 0.1159, Val Loss: 0.1515\n",
      "Epoch 16/100, Loss: 0.1099, Val Loss: 0.1493\n",
      "Epoch 17/100, Loss: 0.1050, Val Loss: 0.1475\n",
      "Epoch 18/100, Loss: 0.1000, Val Loss: 0.1460\n",
      "Epoch 19/100, Loss: 0.0952, Val Loss: 0.1447\n",
      "Epoch 20/100, Loss: 0.0910, Val Loss: 0.1436\n",
      "Epoch 21/100, Loss: 0.0875, Val Loss: 0.1426\n",
      "Epoch 22/100, Loss: 0.0837, Val Loss: 0.1419\n",
      "Epoch 23/100, Loss: 0.0799, Val Loss: 0.1414\n",
      "Epoch 24/100, Loss: 0.0766, Val Loss: 0.1409\n",
      "Epoch 25/100, Loss: 0.0741, Val Loss: 0.1402\n",
      "Epoch 26/100, Loss: 0.0716, Val Loss: 0.1396\n",
      "Epoch 27/100, Loss: 0.0680, Val Loss: 0.1393\n",
      "Epoch 28/100, Loss: 0.0637, Val Loss: 0.1388\n",
      "Epoch 29/100, Loss: 0.0591, Val Loss: 0.1384\n",
      "Epoch 30/100, Loss: 0.0547, Val Loss: 0.1382\n",
      "Epoch 31/100, Loss: 0.0523, Val Loss: 0.1380\n",
      "Epoch 32/100, Loss: 0.0505, Val Loss: 0.1383\n",
      "Epoch 33/100, Loss: 0.0480, Val Loss: 0.1385\n",
      "Epoch 34/100, Loss: 0.0457, Val Loss: 0.1386\n",
      "Epoch 35/100, Loss: 0.0437, Val Loss: 0.1389\n",
      "Epoch 36/100, Loss: 0.0419, Val Loss: 0.1392\n",
      "Epoch 37/100, Loss: 0.0401, Val Loss: 0.1396\n",
      "Epoch 38/100, Loss: 0.0380, Val Loss: 0.1399\n",
      "Epoch 39/100, Loss: 0.0353, Val Loss: 0.1401\n",
      "Epoch 40/100, Loss: 0.0326, Val Loss: 0.1399\n",
      "Epoch 41/100, Loss: 0.0317, Val Loss: 0.1397\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=128, units_per_layer=32, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.2444, Val Loss: 0.1427\n",
      "Epoch 2/100, Loss: 0.1931, Val Loss: 0.1379\n",
      "Epoch 3/100, Loss: 0.1838, Val Loss: 0.1352\n",
      "Epoch 4/100, Loss: 0.1738, Val Loss: 0.1338\n",
      "Epoch 5/100, Loss: 0.1632, Val Loss: 0.1327\n",
      "Epoch 6/100, Loss: 0.1520, Val Loss: 0.1319\n",
      "Epoch 7/100, Loss: 0.1404, Val Loss: 0.1313\n",
      "Epoch 8/100, Loss: 0.1291, Val Loss: 0.1302\n",
      "Epoch 9/100, Loss: 0.1190, Val Loss: 0.1289\n",
      "Epoch 10/100, Loss: 0.1098, Val Loss: 0.1280\n",
      "Epoch 11/100, Loss: 0.1014, Val Loss: 0.1266\n",
      "Epoch 12/100, Loss: 0.0929, Val Loss: 0.1253\n",
      "Epoch 13/100, Loss: 0.0844, Val Loss: 0.1239\n",
      "Epoch 14/100, Loss: 0.0785, Val Loss: 0.1227\n",
      "Epoch 15/100, Loss: 0.0748, Val Loss: 0.1215\n",
      "Epoch 16/100, Loss: 0.0711, Val Loss: 0.1205\n",
      "Epoch 17/100, Loss: 0.0665, Val Loss: 0.1200\n",
      "Epoch 18/100, Loss: 0.0616, Val Loss: 0.1207\n",
      "Epoch 19/100, Loss: 0.0574, Val Loss: 0.1218\n",
      "Epoch 20/100, Loss: 0.0542, Val Loss: 0.1228\n",
      "Epoch 21/100, Loss: 0.0516, Val Loss: 0.1234\n",
      "Epoch 22/100, Loss: 0.0494, Val Loss: 0.1237\n",
      "Epoch 23/100, Loss: 0.0465, Val Loss: 0.1236\n",
      "Epoch 24/100, Loss: 0.0434, Val Loss: 0.1227\n",
      "Epoch 25/100, Loss: 0.0405, Val Loss: 0.1216\n",
      "Epoch 26/100, Loss: 0.0381, Val Loss: 0.1212\n",
      "Epoch 27/100, Loss: 0.0353, Val Loss: 0.1210\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=128, units_per_layer=64, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2570, Val Loss: 0.1870\n",
      "Epoch 2/100, Loss: 0.2150, Val Loss: 0.1676\n",
      "Epoch 3/100, Loss: 0.1899, Val Loss: 0.1543\n",
      "Epoch 4/100, Loss: 0.1683, Val Loss: 0.1468\n",
      "Epoch 5/100, Loss: 0.1546, Val Loss: 0.1419\n",
      "Epoch 6/100, Loss: 0.1437, Val Loss: 0.1379\n",
      "Epoch 7/100, Loss: 0.1319, Val Loss: 0.1336\n",
      "Epoch 8/100, Loss: 0.1199, Val Loss: 0.1295\n",
      "Epoch 9/100, Loss: 0.1079, Val Loss: 0.1266\n",
      "Epoch 10/100, Loss: 0.0951, Val Loss: 0.1282\n",
      "Epoch 11/100, Loss: 0.0840, Val Loss: 0.1299\n",
      "Epoch 12/100, Loss: 0.0757, Val Loss: 0.1299\n",
      "Epoch 13/100, Loss: 0.0715, Val Loss: 0.1287\n",
      "Epoch 14/100, Loss: 0.0660, Val Loss: 0.1264\n",
      "Epoch 15/100, Loss: 0.0577, Val Loss: 0.1242\n",
      "Epoch 16/100, Loss: 0.0497, Val Loss: 0.1232\n",
      "Epoch 17/100, Loss: 0.0434, Val Loss: 0.1231\n",
      "Epoch 18/100, Loss: 0.0447, Val Loss: 0.1233\n",
      "Epoch 19/100, Loss: 0.0445, Val Loss: 0.1245\n",
      "Epoch 20/100, Loss: 0.0417, Val Loss: 0.1259\n",
      "Epoch 21/100, Loss: 0.0374, Val Loss: 0.1272\n",
      "Epoch 22/100, Loss: 0.0333, Val Loss: 0.1277\n",
      "Epoch 23/100, Loss: 0.0313, Val Loss: 0.1270\n",
      "Epoch 24/100, Loss: 0.0302, Val Loss: 0.1257\n",
      "Epoch 25/100, Loss: 0.0281, Val Loss: 0.1243\n",
      "Epoch 26/100, Loss: 0.0253, Val Loss: 0.1234\n",
      "Epoch 27/100, Loss: 0.0223, Val Loss: 0.1228\n",
      "Epoch 28/100, Loss: 0.0216, Val Loss: 0.1230\n",
      "Epoch 29/100, Loss: 0.0211, Val Loss: 0.1239\n",
      "Epoch 30/100, Loss: 0.0198, Val Loss: 0.1252\n",
      "Epoch 31/100, Loss: 0.0174, Val Loss: 0.1265\n",
      "Epoch 32/100, Loss: 0.0166, Val Loss: 0.1271\n",
      "Epoch 33/100, Loss: 0.0162, Val Loss: 0.1274\n",
      "Epoch 34/100, Loss: 0.0160, Val Loss: 0.1275\n",
      "Epoch 35/100, Loss: 0.0140, Val Loss: 0.1277\n",
      "Epoch 36/100, Loss: 0.0125, Val Loss: 0.1283\n",
      "Epoch 37/100, Loss: 0.0118, Val Loss: 0.1291\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=128, units_per_layer=64, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.4098, Val Loss: 0.3915\n",
      "Epoch 2/100, Loss: 0.3945, Val Loss: 0.3560\n",
      "Epoch 3/100, Loss: 0.3577, Val Loss: 0.3194\n",
      "Epoch 4/100, Loss: 0.3198, Val Loss: 0.2824\n",
      "Epoch 5/100, Loss: 0.2829, Val Loss: 0.2485\n",
      "Epoch 6/100, Loss: 0.2495, Val Loss: 0.2175\n",
      "Epoch 7/100, Loss: 0.2215, Val Loss: 0.1906\n",
      "Epoch 8/100, Loss: 0.1993, Val Loss: 0.1726\n",
      "Epoch 9/100, Loss: 0.1832, Val Loss: 0.1600\n",
      "Epoch 10/100, Loss: 0.1729, Val Loss: 0.1528\n",
      "Epoch 11/100, Loss: 0.1687, Val Loss: 0.1528\n",
      "Epoch 12/100, Loss: 0.1640, Val Loss: 0.1523\n",
      "Epoch 13/100, Loss: 0.1565, Val Loss: 0.1507\n",
      "Epoch 14/100, Loss: 0.1479, Val Loss: 0.1481\n",
      "Epoch 15/100, Loss: 0.1381, Val Loss: 0.1476\n",
      "Epoch 16/100, Loss: 0.1285, Val Loss: 0.1482\n",
      "Epoch 17/100, Loss: 0.1194, Val Loss: 0.1483\n",
      "Epoch 18/100, Loss: 0.1095, Val Loss: 0.1473\n",
      "Epoch 19/100, Loss: 0.1027, Val Loss: 0.1449\n",
      "Epoch 20/100, Loss: 0.0962, Val Loss: 0.1415\n",
      "Epoch 21/100, Loss: 0.0890, Val Loss: 0.1381\n",
      "Epoch 22/100, Loss: 0.0813, Val Loss: 0.1345\n",
      "Epoch 23/100, Loss: 0.0744, Val Loss: 0.1311\n",
      "Epoch 24/100, Loss: 0.0684, Val Loss: 0.1280\n",
      "Epoch 25/100, Loss: 0.0631, Val Loss: 0.1271\n",
      "Epoch 26/100, Loss: 0.0589, Val Loss: 0.1270\n",
      "Epoch 27/100, Loss: 0.0550, Val Loss: 0.1275\n",
      "Epoch 28/100, Loss: 0.0510, Val Loss: 0.1286\n",
      "Epoch 29/100, Loss: 0.0457, Val Loss: 0.1301\n",
      "Epoch 30/100, Loss: 0.0413, Val Loss: 0.1316\n",
      "Epoch 31/100, Loss: 0.0395, Val Loss: 0.1323\n",
      "Epoch 32/100, Loss: 0.0405, Val Loss: 0.1324\n",
      "Epoch 33/100, Loss: 0.0394, Val Loss: 0.1316\n",
      "Epoch 34/100, Loss: 0.0365, Val Loss: 0.1302\n",
      "Epoch 35/100, Loss: 0.0332, Val Loss: 0.1285\n",
      "Epoch 36/100, Loss: 0.0296, Val Loss: 0.1269\n",
      "Epoch 37/100, Loss: 0.0277, Val Loss: 0.1257\n",
      "Epoch 38/100, Loss: 0.0257, Val Loss: 0.1251\n",
      "Epoch 39/100, Loss: 0.0246, Val Loss: 0.1250\n",
      "Epoch 40/100, Loss: 0.0245, Val Loss: 0.1252\n",
      "Epoch 41/100, Loss: 0.0242, Val Loss: 0.1254\n",
      "Epoch 42/100, Loss: 0.0224, Val Loss: 0.1255\n",
      "Epoch 43/100, Loss: 0.0196, Val Loss: 0.1257\n",
      "Epoch 44/100, Loss: 0.0189, Val Loss: 0.1261\n",
      "Epoch 45/100, Loss: 0.0183, Val Loss: 0.1269\n",
      "Epoch 46/100, Loss: 0.0171, Val Loss: 0.1275\n",
      "Epoch 47/100, Loss: 0.0155, Val Loss: 0.1274\n",
      "Epoch 48/100, Loss: 0.0147, Val Loss: 0.1266\n",
      "Epoch 49/100, Loss: 0.0136, Val Loss: 0.1256\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=128, units_per_layer=128, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.4053, Val Loss: 0.2627\n",
      "Epoch 2/100, Loss: 0.2933, Val Loss: 0.2048\n",
      "Epoch 3/100, Loss: 0.2305, Val Loss: 0.1783\n",
      "Epoch 4/100, Loss: 0.1947, Val Loss: 0.1755\n",
      "Epoch 5/100, Loss: 0.1719, Val Loss: 0.1835\n",
      "Epoch 6/100, Loss: 0.1619, Val Loss: 0.1944\n",
      "Epoch 7/100, Loss: 0.1545, Val Loss: 0.1985\n",
      "Epoch 8/100, Loss: 0.1434, Val Loss: 0.1962\n",
      "Epoch 9/100, Loss: 0.1269, Val Loss: 0.1865\n",
      "Epoch 10/100, Loss: 0.1041, Val Loss: 0.1716\n",
      "Epoch 11/100, Loss: 0.0839, Val Loss: 0.1584\n",
      "Epoch 12/100, Loss: 0.0742, Val Loss: 0.1498\n",
      "Epoch 13/100, Loss: 0.0679, Val Loss: 0.1437\n",
      "Epoch 14/100, Loss: 0.0630, Val Loss: 0.1413\n",
      "Epoch 15/100, Loss: 0.0608, Val Loss: 0.1423\n",
      "Epoch 16/100, Loss: 0.0567, Val Loss: 0.1461\n",
      "Epoch 17/100, Loss: 0.0498, Val Loss: 0.1532\n",
      "Epoch 18/100, Loss: 0.0489, Val Loss: 0.1577\n",
      "Epoch 19/100, Loss: 0.0517, Val Loss: 0.1584\n",
      "Epoch 20/100, Loss: 0.0499, Val Loss: 0.1543\n",
      "Epoch 21/100, Loss: 0.0429, Val Loss: 0.1475\n",
      "Epoch 22/100, Loss: 0.0371, Val Loss: 0.1406\n",
      "Epoch 23/100, Loss: 0.0311, Val Loss: 0.1367\n",
      "Epoch 24/100, Loss: 0.0305, Val Loss: 0.1378\n",
      "Epoch 25/100, Loss: 0.0326, Val Loss: 0.1398\n",
      "Epoch 26/100, Loss: 0.0309, Val Loss: 0.1442\n",
      "Epoch 27/100, Loss: 0.0293, Val Loss: 0.1473\n",
      "Epoch 28/100, Loss: 0.0275, Val Loss: 0.1481\n",
      "Epoch 29/100, Loss: 0.0262, Val Loss: 0.1446\n",
      "Epoch 30/100, Loss: 0.0212, Val Loss: 0.1393\n",
      "Epoch 31/100, Loss: 0.0171, Val Loss: 0.1353\n",
      "Epoch 32/100, Loss: 0.0173, Val Loss: 0.1342\n",
      "Epoch 33/100, Loss: 0.0161, Val Loss: 0.1359\n",
      "Epoch 34/100, Loss: 0.0156, Val Loss: 0.1374\n",
      "Epoch 35/100, Loss: 0.0167, Val Loss: 0.1382\n",
      "Epoch 36/100, Loss: 0.0169, Val Loss: 0.1377\n",
      "Epoch 37/100, Loss: 0.0136, Val Loss: 0.1378\n",
      "Epoch 38/100, Loss: 0.0129, Val Loss: 0.1396\n",
      "Epoch 39/100, Loss: 0.0131, Val Loss: 0.1415\n",
      "Epoch 40/100, Loss: 0.0131, Val Loss: 0.1424\n",
      "Epoch 41/100, Loss: 0.0130, Val Loss: 0.1411\n",
      "Epoch 42/100, Loss: 0.0121, Val Loss: 0.1380\n",
      "Early stopping!\n",
      "Training with lr=0.001, batch_size=128, units_per_layer=128, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.2655, Val Loss: 0.1767\n",
      "Epoch 2/100, Loss: 0.1880, Val Loss: 0.1740\n",
      "Epoch 3/100, Loss: 0.1578, Val Loss: 0.1680\n",
      "Epoch 4/100, Loss: 0.1349, Val Loss: 0.1572\n",
      "Epoch 5/100, Loss: 0.1120, Val Loss: 0.1455\n",
      "Epoch 6/100, Loss: 0.0902, Val Loss: 0.1368\n",
      "Epoch 7/100, Loss: 0.0738, Val Loss: 0.1321\n",
      "Epoch 8/100, Loss: 0.0726, Val Loss: 0.1290\n",
      "Epoch 9/100, Loss: 0.0686, Val Loss: 0.1260\n",
      "Epoch 10/100, Loss: 0.0586, Val Loss: 0.1283\n",
      "Epoch 11/100, Loss: 0.0559, Val Loss: 0.1332\n",
      "Epoch 12/100, Loss: 0.0552, Val Loss: 0.1341\n",
      "Epoch 13/100, Loss: 0.0502, Val Loss: 0.1311\n",
      "Epoch 14/100, Loss: 0.0424, Val Loss: 0.1282\n",
      "Epoch 15/100, Loss: 0.0375, Val Loss: 0.1270\n",
      "Epoch 16/100, Loss: 0.0341, Val Loss: 0.1290\n",
      "Epoch 17/100, Loss: 0.0286, Val Loss: 0.1315\n",
      "Epoch 18/100, Loss: 0.0292, Val Loss: 0.1334\n",
      "Epoch 19/100, Loss: 0.0301, Val Loss: 0.1339\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=32, units_per_layer=32, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.3136, Val Loss: 0.2137\n",
      "Epoch 2/100, Loss: 0.2087, Val Loss: 0.2446\n",
      "Epoch 3/100, Loss: 0.2026, Val Loss: 0.1511\n",
      "Epoch 4/100, Loss: 0.0982, Val Loss: 0.1403\n",
      "Epoch 5/100, Loss: 0.1138, Val Loss: 0.1377\n",
      "Epoch 6/100, Loss: 0.0920, Val Loss: 0.1402\n",
      "Epoch 7/100, Loss: 0.1084, Val Loss: 0.1347\n",
      "Epoch 8/100, Loss: 0.0845, Val Loss: 0.1477\n",
      "Epoch 9/100, Loss: 0.0968, Val Loss: 0.1508\n",
      "Epoch 10/100, Loss: 0.0845, Val Loss: 0.1665\n",
      "Epoch 11/100, Loss: 0.0791, Val Loss: 0.1823\n",
      "Epoch 12/100, Loss: 0.0763, Val Loss: 0.1472\n",
      "Epoch 13/100, Loss: 0.0543, Val Loss: 0.1292\n",
      "Epoch 14/100, Loss: 0.0522, Val Loss: 0.1215\n",
      "Epoch 15/100, Loss: 0.0453, Val Loss: 0.1208\n",
      "Epoch 16/100, Loss: 0.0395, Val Loss: 0.1227\n",
      "Epoch 17/100, Loss: 0.0336, Val Loss: 0.1285\n",
      "Epoch 18/100, Loss: 0.0297, Val Loss: 0.1350\n",
      "Epoch 19/100, Loss: 0.0316, Val Loss: 0.1342\n",
      "Epoch 20/100, Loss: 0.0298, Val Loss: 0.1318\n",
      "Epoch 21/100, Loss: 0.0272, Val Loss: 0.1310\n",
      "Epoch 22/100, Loss: 0.0353, Val Loss: 0.1371\n",
      "Epoch 23/100, Loss: 0.0416, Val Loss: 0.1460\n",
      "Epoch 24/100, Loss: 0.0301, Val Loss: 0.1497\n",
      "Epoch 25/100, Loss: 0.0350, Val Loss: 0.1462\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=32, units_per_layer=32, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.2274, Val Loss: 0.2017\n",
      "Epoch 2/100, Loss: 0.2132, Val Loss: 0.2396\n",
      "Epoch 3/100, Loss: 0.1847, Val Loss: 0.1793\n",
      "Epoch 4/100, Loss: 0.1313, Val Loss: 0.1426\n",
      "Epoch 5/100, Loss: 0.0970, Val Loss: 0.1369\n",
      "Epoch 6/100, Loss: 0.0788, Val Loss: 0.1446\n",
      "Epoch 7/100, Loss: 0.0990, Val Loss: 0.1260\n",
      "Epoch 8/100, Loss: 0.0735, Val Loss: 0.1092\n",
      "Epoch 9/100, Loss: 0.0710, Val Loss: 0.1225\n",
      "Epoch 10/100, Loss: 0.0803, Val Loss: 0.1326\n",
      "Epoch 11/100, Loss: 0.0760, Val Loss: 0.1357\n",
      "Epoch 12/100, Loss: 0.0781, Val Loss: 0.1320\n",
      "Epoch 13/100, Loss: 0.0569, Val Loss: 0.1223\n",
      "Epoch 14/100, Loss: 0.0440, Val Loss: 0.1133\n",
      "Epoch 15/100, Loss: 0.0396, Val Loss: 0.1075\n",
      "Epoch 16/100, Loss: 0.0335, Val Loss: 0.0995\n",
      "Epoch 17/100, Loss: 0.0364, Val Loss: 0.0939\n",
      "Epoch 18/100, Loss: 0.0375, Val Loss: 0.0945\n",
      "Epoch 19/100, Loss: 0.0460, Val Loss: 0.0950\n",
      "Epoch 20/100, Loss: 0.0391, Val Loss: 0.0955\n",
      "Epoch 21/100, Loss: 0.0315, Val Loss: 0.0991\n",
      "Epoch 22/100, Loss: 0.0250, Val Loss: 0.1026\n",
      "Epoch 23/100, Loss: 0.0209, Val Loss: 0.1057\n",
      "Epoch 24/100, Loss: 0.0248, Val Loss: 0.1123\n",
      "Epoch 25/100, Loss: 0.0318, Val Loss: 0.1068\n",
      "Epoch 26/100, Loss: 0.0316, Val Loss: 0.1037\n",
      "Epoch 27/100, Loss: 0.0277, Val Loss: 0.1045\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=32, units_per_layer=64, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2401, Val Loss: 0.3313\n",
      "Epoch 2/100, Loss: 0.2249, Val Loss: 0.1555\n",
      "Epoch 3/100, Loss: 0.1386, Val Loss: 0.1649\n",
      "Epoch 4/100, Loss: 0.1547, Val Loss: 0.1397\n",
      "Epoch 5/100, Loss: 0.1292, Val Loss: 0.1777\n",
      "Epoch 6/100, Loss: 0.1314, Val Loss: 0.1423\n",
      "Epoch 7/100, Loss: 0.0713, Val Loss: 0.1078\n",
      "Epoch 8/100, Loss: 0.0789, Val Loss: 0.0989\n",
      "Epoch 9/100, Loss: 0.0853, Val Loss: 0.0872\n",
      "Epoch 10/100, Loss: 0.0859, Val Loss: 0.0900\n",
      "Epoch 11/100, Loss: 0.0625, Val Loss: 0.1021\n",
      "Epoch 12/100, Loss: 0.0728, Val Loss: 0.1140\n",
      "Epoch 13/100, Loss: 0.0527, Val Loss: 0.1216\n",
      "Epoch 14/100, Loss: 0.0519, Val Loss: 0.1229\n",
      "Epoch 15/100, Loss: 0.0458, Val Loss: 0.1204\n",
      "Epoch 16/100, Loss: 0.0512, Val Loss: 0.1154\n",
      "Epoch 17/100, Loss: 0.0362, Val Loss: 0.1077\n",
      "Epoch 18/100, Loss: 0.0461, Val Loss: 0.1046\n",
      "Epoch 19/100, Loss: 0.0286, Val Loss: 0.1038\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=32, units_per_layer=64, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.3058, Val Loss: 0.2890\n",
      "Epoch 2/100, Loss: 0.2521, Val Loss: 0.1476\n",
      "Epoch 3/100, Loss: 0.1649, Val Loss: 0.1490\n",
      "Epoch 4/100, Loss: 0.1162, Val Loss: 0.1618\n",
      "Epoch 5/100, Loss: 0.1120, Val Loss: 0.1626\n",
      "Epoch 6/100, Loss: 0.0881, Val Loss: 0.1119\n",
      "Epoch 7/100, Loss: 0.0917, Val Loss: 0.1045\n",
      "Epoch 8/100, Loss: 0.0636, Val Loss: 0.1107\n",
      "Epoch 9/100, Loss: 0.0544, Val Loss: 0.1274\n",
      "Epoch 10/100, Loss: 0.0626, Val Loss: 0.1355\n",
      "Epoch 11/100, Loss: 0.0530, Val Loss: 0.1359\n",
      "Epoch 12/100, Loss: 0.0628, Val Loss: 0.1254\n",
      "Epoch 13/100, Loss: 0.0602, Val Loss: 0.1193\n",
      "Epoch 14/100, Loss: 0.0697, Val Loss: 0.1162\n",
      "Epoch 15/100, Loss: 0.0542, Val Loss: 0.1101\n",
      "Epoch 16/100, Loss: 0.0492, Val Loss: 0.1215\n",
      "Epoch 17/100, Loss: 0.0490, Val Loss: 0.1218\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=32, units_per_layer=128, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.5551, Val Loss: 0.8202\n",
      "Epoch 2/100, Loss: 0.5955, Val Loss: 0.1591\n",
      "Epoch 3/100, Loss: 0.2157, Val Loss: 0.2585\n",
      "Epoch 4/100, Loss: 0.2728, Val Loss: 0.2355\n",
      "Epoch 5/100, Loss: 0.2339, Val Loss: 0.1781\n",
      "Epoch 6/100, Loss: 0.1524, Val Loss: 0.1528\n",
      "Epoch 7/100, Loss: 0.1371, Val Loss: 0.1559\n",
      "Epoch 8/100, Loss: 0.1061, Val Loss: 0.1362\n",
      "Epoch 9/100, Loss: 0.0921, Val Loss: 0.1323\n",
      "Epoch 10/100, Loss: 0.1008, Val Loss: 0.1246\n",
      "Epoch 11/100, Loss: 0.1047, Val Loss: 0.1160\n",
      "Epoch 12/100, Loss: 0.0777, Val Loss: 0.1237\n",
      "Epoch 13/100, Loss: 0.0621, Val Loss: 0.1371\n",
      "Epoch 14/100, Loss: 0.0656, Val Loss: 0.1364\n",
      "Epoch 15/100, Loss: 0.0501, Val Loss: 0.1262\n",
      "Epoch 16/100, Loss: 0.0394, Val Loss: 0.1150\n",
      "Epoch 17/100, Loss: 0.0433, Val Loss: 0.1068\n",
      "Epoch 18/100, Loss: 0.0350, Val Loss: 0.1099\n",
      "Epoch 19/100, Loss: 0.0341, Val Loss: 0.1127\n",
      "Epoch 20/100, Loss: 0.0364, Val Loss: 0.1141\n",
      "Epoch 21/100, Loss: 0.0398, Val Loss: 0.1056\n",
      "Epoch 22/100, Loss: 0.0425, Val Loss: 0.1033\n",
      "Epoch 23/100, Loss: 0.0363, Val Loss: 0.1016\n",
      "Epoch 24/100, Loss: 0.0442, Val Loss: 0.1105\n",
      "Epoch 25/100, Loss: 0.0408, Val Loss: 0.1129\n",
      "Epoch 26/100, Loss: 0.0470, Val Loss: 0.1093\n",
      "Epoch 27/100, Loss: 0.0383, Val Loss: 0.1098\n",
      "Epoch 28/100, Loss: 0.0377, Val Loss: 0.1068\n",
      "Epoch 29/100, Loss: 0.0375, Val Loss: 0.1104\n",
      "Epoch 30/100, Loss: 0.0233, Val Loss: 0.1211\n",
      "Epoch 31/100, Loss: 0.0485, Val Loss: 0.1182\n",
      "Epoch 32/100, Loss: 0.0283, Val Loss: 0.1122\n",
      "Epoch 33/100, Loss: 0.0498, Val Loss: 0.1118\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=32, units_per_layer=128, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.4082, Val Loss: 0.3767\n",
      "Epoch 2/100, Loss: 0.2505, Val Loss: 0.2062\n",
      "Epoch 3/100, Loss: 0.2371, Val Loss: 0.1792\n",
      "Epoch 4/100, Loss: 0.1834, Val Loss: 0.1762\n",
      "Epoch 5/100, Loss: 0.1277, Val Loss: 0.1431\n",
      "Epoch 6/100, Loss: 0.0840, Val Loss: 0.1291\n",
      "Epoch 7/100, Loss: 0.0860, Val Loss: 0.1122\n",
      "Epoch 8/100, Loss: 0.0930, Val Loss: 0.1160\n",
      "Epoch 9/100, Loss: 0.0773, Val Loss: 0.1334\n",
      "Epoch 10/100, Loss: 0.0633, Val Loss: 0.1411\n",
      "Epoch 11/100, Loss: 0.0736, Val Loss: 0.1392\n",
      "Epoch 12/100, Loss: 0.0811, Val Loss: 0.1250\n",
      "Epoch 13/100, Loss: 0.0577, Val Loss: 0.1125\n",
      "Epoch 14/100, Loss: 0.0651, Val Loss: 0.1058\n",
      "Epoch 15/100, Loss: 0.0496, Val Loss: 0.1061\n",
      "Epoch 16/100, Loss: 0.0429, Val Loss: 0.1062\n",
      "Epoch 17/100, Loss: 0.0420, Val Loss: 0.1052\n",
      "Epoch 18/100, Loss: 0.0404, Val Loss: 0.1116\n",
      "Epoch 19/100, Loss: 0.0519, Val Loss: 0.1143\n",
      "Epoch 20/100, Loss: 0.0395, Val Loss: 0.1148\n",
      "Epoch 21/100, Loss: 0.0509, Val Loss: 0.1105\n",
      "Epoch 22/100, Loss: 0.0372, Val Loss: 0.1128\n",
      "Epoch 23/100, Loss: 0.0380, Val Loss: 0.1164\n",
      "Epoch 24/100, Loss: 0.0312, Val Loss: 0.1201\n",
      "Epoch 25/100, Loss: 0.0394, Val Loss: 0.1204\n",
      "Epoch 26/100, Loss: 0.0370, Val Loss: 0.1219\n",
      "Epoch 27/100, Loss: 0.0396, Val Loss: 0.1188\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=64, units_per_layer=32, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2333, Val Loss: 0.1582\n",
      "Epoch 2/100, Loss: 0.1304, Val Loss: 0.1707\n",
      "Epoch 3/100, Loss: 0.1163, Val Loss: 0.1350\n",
      "Epoch 4/100, Loss: 0.0820, Val Loss: 0.1345\n",
      "Epoch 5/100, Loss: 0.0705, Val Loss: 0.1483\n",
      "Epoch 6/100, Loss: 0.0487, Val Loss: 0.1501\n",
      "Epoch 7/100, Loss: 0.0423, Val Loss: 0.1390\n",
      "Epoch 8/100, Loss: 0.0373, Val Loss: 0.1357\n",
      "Epoch 9/100, Loss: 0.0368, Val Loss: 0.1417\n",
      "Epoch 10/100, Loss: 0.0423, Val Loss: 0.1467\n",
      "Epoch 11/100, Loss: 0.0344, Val Loss: 0.1438\n",
      "Epoch 12/100, Loss: 0.0244, Val Loss: 0.1448\n",
      "Epoch 13/100, Loss: 0.0273, Val Loss: 0.1402\n",
      "Epoch 14/100, Loss: 0.0252, Val Loss: 0.1337\n",
      "Epoch 15/100, Loss: 0.0269, Val Loss: 0.1330\n",
      "Epoch 16/100, Loss: 0.0273, Val Loss: 0.1291\n",
      "Epoch 17/100, Loss: 0.0210, Val Loss: 0.1330\n",
      "Epoch 18/100, Loss: 0.0233, Val Loss: 0.1316\n",
      "Epoch 19/100, Loss: 0.0242, Val Loss: 0.1305\n",
      "Epoch 20/100, Loss: 0.0192, Val Loss: 0.1337\n",
      "Epoch 21/100, Loss: 0.0199, Val Loss: 0.1297\n",
      "Epoch 22/100, Loss: 0.0206, Val Loss: 0.1234\n",
      "Epoch 23/100, Loss: 0.0216, Val Loss: 0.1265\n",
      "Epoch 24/100, Loss: 0.0200, Val Loss: 0.1344\n",
      "Epoch 25/100, Loss: 0.0217, Val Loss: 0.1306\n",
      "Epoch 26/100, Loss: 0.0158, Val Loss: 0.1293\n",
      "Epoch 27/100, Loss: 0.0191, Val Loss: 0.1296\n",
      "Epoch 28/100, Loss: 0.0164, Val Loss: 0.1259\n",
      "Epoch 29/100, Loss: 0.0118, Val Loss: 0.1261\n",
      "Epoch 30/100, Loss: 0.0133, Val Loss: 0.1270\n",
      "Epoch 31/100, Loss: 0.0163, Val Loss: 0.1252\n",
      "Epoch 32/100, Loss: 0.0143, Val Loss: 0.1236\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=64, units_per_layer=32, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.3807, Val Loss: 0.2254\n",
      "Epoch 2/100, Loss: 0.2469, Val Loss: 0.1775\n",
      "Epoch 3/100, Loss: 0.1783, Val Loss: 0.1769\n",
      "Epoch 4/100, Loss: 0.1505, Val Loss: 0.1887\n",
      "Epoch 5/100, Loss: 0.1289, Val Loss: 0.1669\n",
      "Epoch 6/100, Loss: 0.0820, Val Loss: 0.1317\n",
      "Epoch 7/100, Loss: 0.0605, Val Loss: 0.1177\n",
      "Epoch 8/100, Loss: 0.0687, Val Loss: 0.1123\n",
      "Epoch 9/100, Loss: 0.0622, Val Loss: 0.1149\n",
      "Epoch 10/100, Loss: 0.0454, Val Loss: 0.1269\n",
      "Epoch 11/100, Loss: 0.0442, Val Loss: 0.1322\n",
      "Epoch 12/100, Loss: 0.0421, Val Loss: 0.1285\n",
      "Epoch 13/100, Loss: 0.0340, Val Loss: 0.1204\n",
      "Epoch 14/100, Loss: 0.0271, Val Loss: 0.1220\n",
      "Epoch 15/100, Loss: 0.0257, Val Loss: 0.1256\n",
      "Epoch 16/100, Loss: 0.0266, Val Loss: 0.1289\n",
      "Epoch 17/100, Loss: 0.0250, Val Loss: 0.1328\n",
      "Epoch 18/100, Loss: 0.0232, Val Loss: 0.1327\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=64, units_per_layer=64, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2792, Val Loss: 0.2502\n",
      "Epoch 2/100, Loss: 0.2377, Val Loss: 0.1602\n",
      "Epoch 3/100, Loss: 0.1355, Val Loss: 0.1320\n",
      "Epoch 4/100, Loss: 0.1061, Val Loss: 0.1263\n",
      "Epoch 5/100, Loss: 0.0857, Val Loss: 0.1479\n",
      "Epoch 6/100, Loss: 0.0658, Val Loss: 0.1533\n",
      "Epoch 7/100, Loss: 0.0497, Val Loss: 0.1484\n",
      "Epoch 8/100, Loss: 0.0453, Val Loss: 0.1351\n",
      "Epoch 9/100, Loss: 0.0373, Val Loss: 0.1250\n",
      "Epoch 10/100, Loss: 0.0346, Val Loss: 0.1232\n",
      "Epoch 11/100, Loss: 0.0345, Val Loss: 0.1295\n",
      "Epoch 12/100, Loss: 0.0326, Val Loss: 0.1342\n",
      "Epoch 13/100, Loss: 0.0344, Val Loss: 0.1327\n",
      "Epoch 14/100, Loss: 0.0363, Val Loss: 0.1314\n",
      "Epoch 15/100, Loss: 0.0310, Val Loss: 0.1283\n",
      "Epoch 16/100, Loss: 0.0259, Val Loss: 0.1279\n",
      "Epoch 17/100, Loss: 0.0285, Val Loss: 0.1292\n",
      "Epoch 18/100, Loss: 0.0251, Val Loss: 0.1323\n",
      "Epoch 19/100, Loss: 0.0212, Val Loss: 0.1344\n",
      "Epoch 20/100, Loss: 0.0211, Val Loss: 0.1360\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=64, units_per_layer=64, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.5536, Val Loss: 0.2104\n",
      "Epoch 2/100, Loss: 0.2087, Val Loss: 0.3668\n",
      "Epoch 3/100, Loss: 0.2620, Val Loss: 0.2717\n",
      "Epoch 4/100, Loss: 0.1662, Val Loss: 0.1737\n",
      "Epoch 5/100, Loss: 0.1194, Val Loss: 0.1396\n",
      "Epoch 6/100, Loss: 0.1119, Val Loss: 0.1363\n",
      "Epoch 7/100, Loss: 0.0887, Val Loss: 0.1340\n",
      "Epoch 8/100, Loss: 0.0739, Val Loss: 0.1322\n",
      "Epoch 9/100, Loss: 0.0698, Val Loss: 0.1335\n",
      "Epoch 10/100, Loss: 0.0469, Val Loss: 0.1337\n",
      "Epoch 11/100, Loss: 0.0362, Val Loss: 0.1346\n",
      "Epoch 12/100, Loss: 0.0431, Val Loss: 0.1352\n",
      "Epoch 13/100, Loss: 0.0462, Val Loss: 0.1308\n",
      "Epoch 14/100, Loss: 0.0412, Val Loss: 0.1311\n",
      "Epoch 15/100, Loss: 0.0307, Val Loss: 0.1352\n",
      "Epoch 16/100, Loss: 0.0356, Val Loss: 0.1349\n",
      "Epoch 17/100, Loss: 0.0340, Val Loss: 0.1330\n",
      "Epoch 18/100, Loss: 0.0284, Val Loss: 0.1267\n",
      "Epoch 19/100, Loss: 0.0248, Val Loss: 0.1202\n",
      "Epoch 20/100, Loss: 0.0241, Val Loss: 0.1209\n",
      "Epoch 21/100, Loss: 0.0199, Val Loss: 0.1234\n",
      "Epoch 22/100, Loss: 0.0231, Val Loss: 0.1252\n",
      "Epoch 23/100, Loss: 0.0196, Val Loss: 0.1280\n",
      "Epoch 24/100, Loss: 0.0167, Val Loss: 0.1303\n",
      "Epoch 25/100, Loss: 0.0164, Val Loss: 0.1301\n",
      "Epoch 26/100, Loss: 0.0203, Val Loss: 0.1268\n",
      "Epoch 27/100, Loss: 0.0186, Val Loss: 0.1232\n",
      "Epoch 28/100, Loss: 0.0190, Val Loss: 0.1212\n",
      "Epoch 29/100, Loss: 0.0161, Val Loss: 0.1211\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=64, units_per_layer=128, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.3486, Val Loss: 0.9392\n",
      "Epoch 2/100, Loss: 0.8215, Val Loss: 0.4019\n",
      "Epoch 3/100, Loss: 0.3045, Val Loss: 0.1718\n",
      "Epoch 4/100, Loss: 0.2083, Val Loss: 0.2000\n",
      "Epoch 5/100, Loss: 0.2420, Val Loss: 0.1953\n",
      "Epoch 6/100, Loss: 0.2067, Val Loss: 0.1767\n",
      "Epoch 7/100, Loss: 0.1516, Val Loss: 0.1667\n",
      "Epoch 8/100, Loss: 0.1266, Val Loss: 0.1762\n",
      "Epoch 9/100, Loss: 0.1224, Val Loss: 0.1664\n",
      "Epoch 10/100, Loss: 0.0966, Val Loss: 0.1370\n",
      "Epoch 11/100, Loss: 0.0470, Val Loss: 0.1311\n",
      "Epoch 12/100, Loss: 0.0666, Val Loss: 0.1298\n",
      "Epoch 13/100, Loss: 0.0697, Val Loss: 0.1324\n",
      "Epoch 14/100, Loss: 0.0608, Val Loss: 0.1377\n",
      "Epoch 15/100, Loss: 0.0453, Val Loss: 0.1493\n",
      "Epoch 16/100, Loss: 0.0484, Val Loss: 0.1479\n",
      "Epoch 17/100, Loss: 0.0435, Val Loss: 0.1447\n",
      "Epoch 18/100, Loss: 0.0357, Val Loss: 0.1412\n",
      "Epoch 19/100, Loss: 0.0395, Val Loss: 0.1329\n",
      "Epoch 20/100, Loss: 0.0329, Val Loss: 0.1256\n",
      "Epoch 21/100, Loss: 0.0294, Val Loss: 0.1236\n",
      "Epoch 22/100, Loss: 0.0218, Val Loss: 0.1228\n",
      "Epoch 23/100, Loss: 0.0212, Val Loss: 0.1291\n",
      "Epoch 24/100, Loss: 0.0209, Val Loss: 0.1340\n",
      "Epoch 25/100, Loss: 0.0170, Val Loss: 0.1370\n",
      "Epoch 26/100, Loss: 0.0166, Val Loss: 0.1398\n",
      "Epoch 27/100, Loss: 0.0173, Val Loss: 0.1383\n",
      "Epoch 28/100, Loss: 0.0167, Val Loss: 0.1355\n",
      "Epoch 29/100, Loss: 0.0194, Val Loss: 0.1340\n",
      "Epoch 30/100, Loss: 0.0178, Val Loss: 0.1319\n",
      "Epoch 31/100, Loss: 0.0162, Val Loss: 0.1322\n",
      "Epoch 32/100, Loss: 0.0169, Val Loss: 0.1324\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=64, units_per_layer=128, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.2910, Val Loss: 0.7310\n",
      "Epoch 2/100, Loss: 0.6491, Val Loss: 0.2029\n",
      "Epoch 3/100, Loss: 0.2102, Val Loss: 0.1946\n",
      "Epoch 4/100, Loss: 0.2139, Val Loss: 0.2013\n",
      "Epoch 5/100, Loss: 0.2092, Val Loss: 0.1551\n",
      "Epoch 6/100, Loss: 0.1436, Val Loss: 0.1324\n",
      "Epoch 7/100, Loss: 0.0980, Val Loss: 0.1482\n",
      "Epoch 8/100, Loss: 0.1210, Val Loss: 0.1455\n",
      "Epoch 9/100, Loss: 0.0958, Val Loss: 0.1295\n",
      "Epoch 10/100, Loss: 0.0456, Val Loss: 0.1378\n",
      "Epoch 11/100, Loss: 0.0654, Val Loss: 0.1365\n",
      "Epoch 12/100, Loss: 0.0619, Val Loss: 0.1360\n",
      "Epoch 13/100, Loss: 0.0430, Val Loss: 0.1374\n",
      "Epoch 14/100, Loss: 0.0560, Val Loss: 0.1310\n",
      "Epoch 15/100, Loss: 0.0564, Val Loss: 0.1184\n",
      "Epoch 16/100, Loss: 0.0268, Val Loss: 0.1241\n",
      "Epoch 17/100, Loss: 0.0493, Val Loss: 0.1264\n",
      "Epoch 18/100, Loss: 0.0599, Val Loss: 0.1178\n",
      "Epoch 19/100, Loss: 0.0426, Val Loss: 0.1084\n",
      "Epoch 20/100, Loss: 0.0398, Val Loss: 0.1098\n",
      "Epoch 21/100, Loss: 0.0409, Val Loss: 0.1174\n",
      "Epoch 22/100, Loss: 0.0287, Val Loss: 0.1223\n",
      "Epoch 23/100, Loss: 0.0238, Val Loss: 0.1242\n",
      "Epoch 24/100, Loss: 0.0326, Val Loss: 0.1242\n",
      "Epoch 25/100, Loss: 0.0281, Val Loss: 0.1226\n",
      "Epoch 26/100, Loss: 0.0223, Val Loss: 0.1196\n",
      "Epoch 27/100, Loss: 0.0283, Val Loss: 0.1172\n",
      "Epoch 28/100, Loss: 0.0224, Val Loss: 0.1191\n",
      "Epoch 29/100, Loss: 0.0258, Val Loss: 0.1197\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=128, units_per_layer=32, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.4528, Val Loss: 0.2553\n",
      "Epoch 2/100, Loss: 0.2648, Val Loss: 0.1666\n",
      "Epoch 3/100, Loss: 0.1815, Val Loss: 0.1953\n",
      "Epoch 4/100, Loss: 0.1758, Val Loss: 0.1909\n",
      "Epoch 5/100, Loss: 0.1340, Val Loss: 0.1609\n",
      "Epoch 6/100, Loss: 0.1043, Val Loss: 0.1438\n",
      "Epoch 7/100, Loss: 0.0866, Val Loss: 0.1372\n",
      "Epoch 8/100, Loss: 0.0709, Val Loss: 0.1273\n",
      "Epoch 9/100, Loss: 0.0531, Val Loss: 0.1278\n",
      "Epoch 10/100, Loss: 0.0514, Val Loss: 0.1308\n",
      "Epoch 11/100, Loss: 0.0470, Val Loss: 0.1308\n",
      "Epoch 12/100, Loss: 0.0386, Val Loss: 0.1312\n",
      "Epoch 13/100, Loss: 0.0351, Val Loss: 0.1359\n",
      "Epoch 14/100, Loss: 0.0351, Val Loss: 0.1375\n",
      "Epoch 15/100, Loss: 0.0321, Val Loss: 0.1353\n",
      "Epoch 16/100, Loss: 0.0322, Val Loss: 0.1350\n",
      "Epoch 17/100, Loss: 0.0299, Val Loss: 0.1368\n",
      "Epoch 18/100, Loss: 0.0242, Val Loss: 0.1388\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=128, units_per_layer=32, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.3892, Val Loss: 0.2786\n",
      "Epoch 2/100, Loss: 0.2620, Val Loss: 0.1728\n",
      "Epoch 3/100, Loss: 0.1851, Val Loss: 0.1731\n",
      "Epoch 4/100, Loss: 0.1958, Val Loss: 0.1708\n",
      "Epoch 5/100, Loss: 0.1516, Val Loss: 0.1427\n",
      "Epoch 6/100, Loss: 0.0837, Val Loss: 0.1440\n",
      "Epoch 7/100, Loss: 0.0798, Val Loss: 0.1529\n",
      "Epoch 8/100, Loss: 0.0898, Val Loss: 0.1591\n",
      "Epoch 9/100, Loss: 0.0641, Val Loss: 0.1647\n",
      "Epoch 10/100, Loss: 0.0630, Val Loss: 0.1615\n",
      "Epoch 11/100, Loss: 0.0726, Val Loss: 0.1513\n",
      "Epoch 12/100, Loss: 0.0516, Val Loss: 0.1433\n",
      "Epoch 13/100, Loss: 0.0448, Val Loss: 0.1459\n",
      "Epoch 14/100, Loss: 0.0500, Val Loss: 0.1477\n",
      "Epoch 15/100, Loss: 0.0404, Val Loss: 0.1491\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=128, units_per_layer=64, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.4729, Val Loss: 0.2360\n",
      "Epoch 2/100, Loss: 0.1846, Val Loss: 0.2985\n",
      "Epoch 3/100, Loss: 0.1762, Val Loss: 0.2286\n",
      "Epoch 4/100, Loss: 0.1067, Val Loss: 0.1743\n",
      "Epoch 5/100, Loss: 0.0870, Val Loss: 0.1506\n",
      "Epoch 6/100, Loss: 0.0550, Val Loss: 0.1452\n",
      "Epoch 7/100, Loss: 0.0538, Val Loss: 0.1305\n",
      "Epoch 8/100, Loss: 0.0412, Val Loss: 0.1280\n",
      "Epoch 9/100, Loss: 0.0317, Val Loss: 0.1411\n",
      "Epoch 10/100, Loss: 0.0292, Val Loss: 0.1433\n",
      "Epoch 11/100, Loss: 0.0303, Val Loss: 0.1249\n",
      "Epoch 12/100, Loss: 0.0264, Val Loss: 0.1187\n",
      "Epoch 13/100, Loss: 0.0240, Val Loss: 0.1183\n",
      "Epoch 14/100, Loss: 0.0205, Val Loss: 0.1186\n",
      "Epoch 15/100, Loss: 0.0213, Val Loss: 0.1151\n",
      "Epoch 16/100, Loss: 0.0155, Val Loss: 0.1154\n",
      "Epoch 17/100, Loss: 0.0185, Val Loss: 0.1188\n",
      "Epoch 18/100, Loss: 0.0206, Val Loss: 0.1196\n",
      "Epoch 19/100, Loss: 0.0231, Val Loss: 0.1223\n",
      "Epoch 20/100, Loss: 0.0148, Val Loss: 0.1188\n",
      "Epoch 21/100, Loss: 0.0161, Val Loss: 0.1167\n",
      "Epoch 22/100, Loss: 0.0210, Val Loss: 0.1195\n",
      "Epoch 23/100, Loss: 0.0184, Val Loss: 0.1230\n",
      "Epoch 24/100, Loss: 0.0155, Val Loss: 0.1224\n",
      "Epoch 25/100, Loss: 0.0176, Val Loss: 0.1209\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=128, units_per_layer=64, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.3999, Val Loss: 0.1954\n",
      "Epoch 2/100, Loss: 0.1828, Val Loss: 0.2164\n",
      "Epoch 3/100, Loss: 0.1654, Val Loss: 0.1624\n",
      "Epoch 4/100, Loss: 0.1023, Val Loss: 0.1409\n",
      "Epoch 5/100, Loss: 0.0983, Val Loss: 0.1372\n",
      "Epoch 6/100, Loss: 0.0855, Val Loss: 0.1275\n",
      "Epoch 7/100, Loss: 0.0657, Val Loss: 0.1179\n",
      "Epoch 8/100, Loss: 0.0523, Val Loss: 0.1207\n",
      "Epoch 9/100, Loss: 0.0527, Val Loss: 0.1229\n",
      "Epoch 10/100, Loss: 0.0483, Val Loss: 0.1212\n",
      "Epoch 11/100, Loss: 0.0392, Val Loss: 0.1137\n",
      "Epoch 12/100, Loss: 0.0344, Val Loss: 0.1140\n",
      "Epoch 13/100, Loss: 0.0282, Val Loss: 0.1154\n",
      "Epoch 14/100, Loss: 0.0346, Val Loss: 0.1145\n",
      "Epoch 15/100, Loss: 0.0341, Val Loss: 0.1148\n",
      "Epoch 16/100, Loss: 0.0312, Val Loss: 0.1186\n",
      "Epoch 17/100, Loss: 0.0389, Val Loss: 0.1180\n",
      "Epoch 18/100, Loss: 0.0280, Val Loss: 0.1194\n",
      "Epoch 19/100, Loss: 0.0405, Val Loss: 0.1178\n",
      "Epoch 20/100, Loss: 0.0281, Val Loss: 0.1190\n",
      "Epoch 21/100, Loss: 0.0288, Val Loss: 0.1207\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=128, units_per_layer=128, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.4179, Val Loss: 0.7878\n",
      "Epoch 2/100, Loss: 0.7029, Val Loss: 0.3659\n",
      "Epoch 3/100, Loss: 0.3144, Val Loss: 0.1613\n",
      "Epoch 4/100, Loss: 0.1718, Val Loss: 0.1660\n",
      "Epoch 5/100, Loss: 0.1909, Val Loss: 0.1708\n",
      "Epoch 6/100, Loss: 0.1855, Val Loss: 0.1491\n",
      "Epoch 7/100, Loss: 0.1351, Val Loss: 0.1188\n",
      "Epoch 8/100, Loss: 0.0971, Val Loss: 0.1260\n",
      "Epoch 9/100, Loss: 0.1119, Val Loss: 0.1462\n",
      "Epoch 10/100, Loss: 0.1231, Val Loss: 0.1140\n",
      "Epoch 11/100, Loss: 0.0714, Val Loss: 0.1090\n",
      "Epoch 12/100, Loss: 0.0393, Val Loss: 0.1251\n",
      "Epoch 13/100, Loss: 0.0788, Val Loss: 0.1253\n",
      "Epoch 14/100, Loss: 0.0758, Val Loss: 0.1152\n",
      "Epoch 15/100, Loss: 0.0465, Val Loss: 0.1119\n",
      "Epoch 16/100, Loss: 0.0569, Val Loss: 0.1123\n",
      "Epoch 17/100, Loss: 0.0677, Val Loss: 0.1036\n",
      "Epoch 18/100, Loss: 0.0328, Val Loss: 0.1075\n",
      "Epoch 19/100, Loss: 0.0372, Val Loss: 0.1110\n",
      "Epoch 20/100, Loss: 0.0556, Val Loss: 0.1066\n",
      "Epoch 21/100, Loss: 0.0434, Val Loss: 0.1046\n",
      "Epoch 22/100, Loss: 0.0314, Val Loss: 0.1077\n",
      "Epoch 23/100, Loss: 0.0398, Val Loss: 0.1099\n",
      "Epoch 24/100, Loss: 0.0277, Val Loss: 0.1145\n",
      "Epoch 25/100, Loss: 0.0269, Val Loss: 0.1158\n",
      "Epoch 26/100, Loss: 0.0356, Val Loss: 0.1156\n",
      "Epoch 27/100, Loss: 0.0285, Val Loss: 0.1136\n",
      "Early stopping!\n",
      "Training with lr=0.01, batch_size=128, units_per_layer=128, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.2683, Val Loss: 0.6227\n",
      "Epoch 2/100, Loss: 0.5161, Val Loss: 0.1691\n",
      "Epoch 3/100, Loss: 0.1615, Val Loss: 0.1832\n",
      "Epoch 4/100, Loss: 0.2297, Val Loss: 0.1418\n",
      "Epoch 5/100, Loss: 0.1386, Val Loss: 0.1416\n",
      "Epoch 6/100, Loss: 0.0980, Val Loss: 0.1628\n",
      "Epoch 7/100, Loss: 0.1183, Val Loss: 0.1550\n",
      "Epoch 8/100, Loss: 0.0769, Val Loss: 0.1331\n",
      "Epoch 9/100, Loss: 0.0733, Val Loss: 0.1328\n",
      "Epoch 10/100, Loss: 0.0900, Val Loss: 0.1293\n",
      "Epoch 11/100, Loss: 0.0758, Val Loss: 0.1279\n",
      "Epoch 12/100, Loss: 0.0545, Val Loss: 0.1310\n",
      "Epoch 13/100, Loss: 0.0530, Val Loss: 0.1192\n",
      "Epoch 14/100, Loss: 0.0441, Val Loss: 0.1103\n",
      "Epoch 15/100, Loss: 0.0333, Val Loss: 0.1098\n",
      "Epoch 16/100, Loss: 0.0432, Val Loss: 0.1054\n",
      "Epoch 17/100, Loss: 0.0453, Val Loss: 0.0999\n",
      "Epoch 18/100, Loss: 0.0384, Val Loss: 0.0977\n",
      "Epoch 19/100, Loss: 0.0331, Val Loss: 0.1029\n",
      "Epoch 20/100, Loss: 0.0201, Val Loss: 0.1097\n",
      "Epoch 21/100, Loss: 0.0297, Val Loss: 0.1134\n",
      "Epoch 22/100, Loss: 0.0328, Val Loss: 0.1154\n",
      "Epoch 23/100, Loss: 0.0272, Val Loss: 0.1154\n",
      "Epoch 24/100, Loss: 0.0289, Val Loss: 0.1128\n",
      "Epoch 25/100, Loss: 0.0237, Val Loss: 0.1079\n",
      "Epoch 26/100, Loss: 0.0185, Val Loss: 0.1054\n",
      "Epoch 27/100, Loss: 0.0183, Val Loss: 0.1062\n",
      "Epoch 28/100, Loss: 0.0199, Val Loss: 0.1083\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=32, units_per_layer=32, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 1.1645, Val Loss: 0.3606\n",
      "Epoch 2/100, Loss: 0.4563, Val Loss: 0.3627\n",
      "Epoch 3/100, Loss: 0.4337, Val Loss: 0.1712\n",
      "Epoch 4/100, Loss: 0.2508, Val Loss: 0.1494\n",
      "Epoch 5/100, Loss: 0.2073, Val Loss: 0.1633\n",
      "Epoch 6/100, Loss: 0.1534, Val Loss: 0.1643\n",
      "Epoch 7/100, Loss: 0.1839, Val Loss: 0.1882\n",
      "Epoch 8/100, Loss: 0.2199, Val Loss: 0.1948\n",
      "Epoch 9/100, Loss: 0.2053, Val Loss: 0.1814\n",
      "Epoch 10/100, Loss: 0.1955, Val Loss: 0.1631\n",
      "Epoch 11/100, Loss: 0.2035, Val Loss: 0.1530\n",
      "Epoch 12/100, Loss: 0.2064, Val Loss: 0.1556\n",
      "Epoch 13/100, Loss: 0.1987, Val Loss: 0.1644\n",
      "Epoch 14/100, Loss: 0.1786, Val Loss: 0.1673\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=32, units_per_layer=32, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 2.1859, Val Loss: 1.9461\n",
      "Epoch 2/100, Loss: 1.3716, Val Loss: 0.4581\n",
      "Epoch 3/100, Loss: 0.4006, Val Loss: 0.3534\n",
      "Epoch 4/100, Loss: 0.3309, Val Loss: 0.1913\n",
      "Epoch 5/100, Loss: 0.2052, Val Loss: 0.1593\n",
      "Epoch 6/100, Loss: 0.1764, Val Loss: 0.1830\n",
      "Epoch 7/100, Loss: 0.1622, Val Loss: 0.1909\n",
      "Epoch 8/100, Loss: 0.1700, Val Loss: 0.1866\n",
      "Epoch 9/100, Loss: 0.1968, Val Loss: 0.1780\n",
      "Epoch 10/100, Loss: 0.1838, Val Loss: 0.1684\n",
      "Epoch 11/100, Loss: 0.1994, Val Loss: 0.1610\n",
      "Epoch 12/100, Loss: 0.1823, Val Loss: 0.1586\n",
      "Epoch 13/100, Loss: 0.1617, Val Loss: 0.1591\n",
      "Epoch 14/100, Loss: 0.1772, Val Loss: 0.1604\n",
      "Epoch 15/100, Loss: 0.1741, Val Loss: 0.1621\n",
      "Epoch 16/100, Loss: 0.1910, Val Loss: 0.1629\n",
      "Epoch 17/100, Loss: 0.1611, Val Loss: 0.1645\n",
      "Epoch 18/100, Loss: 0.1716, Val Loss: 0.1663\n",
      "Epoch 19/100, Loss: 0.1816, Val Loss: 0.1659\n",
      "Epoch 20/100, Loss: 0.1703, Val Loss: 0.1638\n",
      "Epoch 21/100, Loss: 0.1688, Val Loss: 0.1608\n",
      "Epoch 22/100, Loss: 0.1900, Val Loss: 0.1590\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=32, units_per_layer=64, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 9.9827, Val Loss: 5.0966\n",
      "Epoch 2/100, Loss: 3.0924, Val Loss: 0.2639\n",
      "Epoch 3/100, Loss: 0.2795, Val Loss: 0.1953\n",
      "Epoch 4/100, Loss: 0.2085, Val Loss: 0.1576\n",
      "Epoch 5/100, Loss: 0.1825, Val Loss: 0.1561\n",
      "Epoch 6/100, Loss: 0.1739, Val Loss: 0.1791\n",
      "Epoch 7/100, Loss: 0.1779, Val Loss: 0.2090\n",
      "Epoch 8/100, Loss: 0.2062, Val Loss: 0.2232\n",
      "Epoch 9/100, Loss: 0.2100, Val Loss: 0.2127\n",
      "Epoch 10/100, Loss: 0.2287, Val Loss: 0.1911\n",
      "Epoch 11/100, Loss: 0.1893, Val Loss: 0.1669\n",
      "Epoch 12/100, Loss: 0.1642, Val Loss: 0.1554\n",
      "Epoch 13/100, Loss: 0.1684, Val Loss: 0.1508\n",
      "Epoch 14/100, Loss: 0.1679, Val Loss: 0.1578\n",
      "Epoch 15/100, Loss: 0.1728, Val Loss: 0.1645\n",
      "Epoch 16/100, Loss: 0.1871, Val Loss: 0.1679\n",
      "Epoch 17/100, Loss: 0.2065, Val Loss: 0.1606\n",
      "Epoch 18/100, Loss: 0.1992, Val Loss: 0.1542\n",
      "Epoch 19/100, Loss: 0.1985, Val Loss: 0.1505\n",
      "Epoch 20/100, Loss: 0.1866, Val Loss: 0.1569\n",
      "Epoch 21/100, Loss: 0.1914, Val Loss: 0.1622\n",
      "Epoch 22/100, Loss: 0.1957, Val Loss: 0.1662\n",
      "Epoch 23/100, Loss: 0.2023, Val Loss: 0.1698\n",
      "Epoch 24/100, Loss: 0.2046, Val Loss: 0.1715\n",
      "Epoch 25/100, Loss: 0.1859, Val Loss: 0.1699\n",
      "Epoch 26/100, Loss: 0.1735, Val Loss: 0.1690\n",
      "Epoch 27/100, Loss: 0.1658, Val Loss: 0.1709\n",
      "Epoch 28/100, Loss: 0.1950, Val Loss: 0.1684\n",
      "Epoch 29/100, Loss: 0.1853, Val Loss: 0.1652\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=32, units_per_layer=64, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 8.1431, Val Loss: 9.4338\n",
      "Epoch 2/100, Loss: 5.9619, Val Loss: 0.3545\n",
      "Epoch 3/100, Loss: 0.3380, Val Loss: 0.3121\n",
      "Epoch 4/100, Loss: 0.3261, Val Loss: 0.2245\n",
      "Epoch 5/100, Loss: 0.2253, Val Loss: 0.1602\n",
      "Epoch 6/100, Loss: 0.1775, Val Loss: 0.1563\n",
      "Epoch 7/100, Loss: 0.1747, Val Loss: 0.1857\n",
      "Epoch 8/100, Loss: 0.2176, Val Loss: 0.2170\n",
      "Epoch 9/100, Loss: 0.2182, Val Loss: 0.2275\n",
      "Epoch 10/100, Loss: 0.2369, Val Loss: 0.2120\n",
      "Epoch 11/100, Loss: 0.2196, Val Loss: 0.1870\n",
      "Epoch 12/100, Loss: 0.1870, Val Loss: 0.1624\n",
      "Epoch 13/100, Loss: 0.1737, Val Loss: 0.1518\n",
      "Epoch 14/100, Loss: 0.1810, Val Loss: 0.1523\n",
      "Epoch 15/100, Loss: 0.1789, Val Loss: 0.1559\n",
      "Epoch 16/100, Loss: 0.2013, Val Loss: 0.1572\n",
      "Epoch 17/100, Loss: 0.1733, Val Loss: 0.1570\n",
      "Epoch 18/100, Loss: 0.1749, Val Loss: 0.1573\n",
      "Epoch 19/100, Loss: 0.1901, Val Loss: 0.1571\n",
      "Epoch 20/100, Loss: 0.1943, Val Loss: 0.1542\n",
      "Epoch 21/100, Loss: 0.1890, Val Loss: 0.1500\n",
      "Epoch 22/100, Loss: 0.1943, Val Loss: 0.1563\n",
      "Epoch 23/100, Loss: 0.2102, Val Loss: 0.1658\n",
      "Epoch 24/100, Loss: 0.1871, Val Loss: 0.1817\n",
      "Epoch 25/100, Loss: 0.1876, Val Loss: 0.1884\n",
      "Epoch 26/100, Loss: 0.2020, Val Loss: 0.1849\n",
      "Epoch 27/100, Loss: 0.2258, Val Loss: 0.1712\n",
      "Epoch 28/100, Loss: 0.1706, Val Loss: 0.1596\n",
      "Epoch 29/100, Loss: 0.1907, Val Loss: 0.1515\n",
      "Epoch 30/100, Loss: 0.1795, Val Loss: 0.1528\n",
      "Epoch 31/100, Loss: 0.2012, Val Loss: 0.1568\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=32, units_per_layer=128, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 37.3723, Val Loss: 15.9624\n",
      "Epoch 2/100, Loss: 11.8818, Val Loss: 1.8366\n",
      "Epoch 3/100, Loss: 1.5135, Val Loss: 0.1889\n",
      "Epoch 4/100, Loss: 0.2076, Val Loss: 0.1540\n",
      "Epoch 5/100, Loss: 0.1879, Val Loss: 0.1579\n",
      "Epoch 6/100, Loss: 0.1827, Val Loss: 0.1777\n",
      "Epoch 7/100, Loss: 0.1683, Val Loss: 0.1995\n",
      "Epoch 8/100, Loss: 0.2063, Val Loss: 0.2033\n",
      "Epoch 9/100, Loss: 0.1898, Val Loss: 0.1884\n",
      "Epoch 10/100, Loss: 0.1998, Val Loss: 0.1695\n",
      "Epoch 11/100, Loss: 0.1848, Val Loss: 0.1603\n",
      "Epoch 12/100, Loss: 0.2193, Val Loss: 0.1550\n",
      "Epoch 13/100, Loss: 0.1818, Val Loss: 0.1513\n",
      "Epoch 14/100, Loss: 0.2168, Val Loss: 0.1507\n",
      "Epoch 15/100, Loss: 0.1963, Val Loss: 0.1518\n",
      "Epoch 16/100, Loss: 0.2013, Val Loss: 0.1537\n",
      "Epoch 17/100, Loss: 0.1727, Val Loss: 0.1554\n",
      "Epoch 18/100, Loss: 0.2030, Val Loss: 0.1582\n",
      "Epoch 19/100, Loss: 0.1583, Val Loss: 0.1633\n",
      "Epoch 20/100, Loss: 0.1792, Val Loss: 0.1672\n",
      "Epoch 21/100, Loss: 0.1872, Val Loss: 0.1680\n",
      "Epoch 22/100, Loss: 0.1583, Val Loss: 0.1662\n",
      "Epoch 23/100, Loss: 0.1761, Val Loss: 0.1628\n",
      "Epoch 24/100, Loss: 0.1662, Val Loss: 0.1592\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=32, units_per_layer=128, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 24.5163, Val Loss: 22.9866\n",
      "Epoch 2/100, Loss: 13.5207, Val Loss: 0.3010\n",
      "Epoch 3/100, Loss: 0.2821, Val Loss: 0.1794\n",
      "Epoch 4/100, Loss: 0.1854, Val Loss: 0.1557\n",
      "Epoch 5/100, Loss: 0.1729, Val Loss: 0.1551\n",
      "Epoch 6/100, Loss: 0.1651, Val Loss: 0.1704\n",
      "Epoch 7/100, Loss: 0.1795, Val Loss: 0.1959\n",
      "Epoch 8/100, Loss: 0.2003, Val Loss: 0.2080\n",
      "Epoch 9/100, Loss: 0.1937, Val Loss: 0.1997\n",
      "Epoch 10/100, Loss: 0.1852, Val Loss: 0.1785\n",
      "Epoch 11/100, Loss: 0.1968, Val Loss: 0.1602\n",
      "Epoch 12/100, Loss: 0.1776, Val Loss: 0.1528\n",
      "Epoch 13/100, Loss: 0.1630, Val Loss: 0.1500\n",
      "Epoch 14/100, Loss: 0.1757, Val Loss: 0.1519\n",
      "Epoch 15/100, Loss: 0.2038, Val Loss: 0.1524\n",
      "Epoch 16/100, Loss: 0.1853, Val Loss: 0.1500\n",
      "Epoch 17/100, Loss: 0.1920, Val Loss: 0.1505\n",
      "Epoch 18/100, Loss: 0.2066, Val Loss: 0.1537\n",
      "Epoch 19/100, Loss: 0.1930, Val Loss: 0.1580\n",
      "Epoch 20/100, Loss: 0.1951, Val Loss: 0.1616\n",
      "Epoch 21/100, Loss: 0.1763, Val Loss: 0.1666\n",
      "Epoch 22/100, Loss: 0.1656, Val Loss: 0.1658\n",
      "Epoch 23/100, Loss: 0.1829, Val Loss: 0.1615\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=64, units_per_layer=32, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2450, Val Loss: 3.3961\n",
      "Epoch 2/100, Loss: 3.3070, Val Loss: 0.4982\n",
      "Epoch 3/100, Loss: 0.5558, Val Loss: 0.2647\n",
      "Epoch 4/100, Loss: 0.3222, Val Loss: 0.2294\n",
      "Epoch 5/100, Loss: 0.3022, Val Loss: 0.1738\n",
      "Epoch 6/100, Loss: 0.1827, Val Loss: 0.1523\n",
      "Epoch 7/100, Loss: 0.1809, Val Loss: 0.1525\n",
      "Epoch 8/100, Loss: 0.1808, Val Loss: 0.1529\n",
      "Epoch 9/100, Loss: 0.1808, Val Loss: 0.1533\n",
      "Epoch 10/100, Loss: 0.1807, Val Loss: 0.1538\n",
      "Epoch 11/100, Loss: 0.1806, Val Loss: 0.1544\n",
      "Epoch 12/100, Loss: 0.1805, Val Loss: 0.1551\n",
      "Epoch 13/100, Loss: 0.1804, Val Loss: 0.1558\n",
      "Epoch 14/100, Loss: 0.1803, Val Loss: 0.1566\n",
      "Epoch 15/100, Loss: 0.1802, Val Loss: 0.1575\n",
      "Epoch 16/100, Loss: 0.1801, Val Loss: 0.1584\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=64, units_per_layer=32, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.4234, Val Loss: 4.7745\n",
      "Epoch 2/100, Loss: 5.0427, Val Loss: 0.3317\n",
      "Epoch 3/100, Loss: 0.3593, Val Loss: 0.2172\n",
      "Epoch 4/100, Loss: 0.2465, Val Loss: 0.2219\n",
      "Epoch 5/100, Loss: 0.2497, Val Loss: 0.2043\n",
      "Epoch 6/100, Loss: 0.2334, Val Loss: 0.1809\n",
      "Epoch 7/100, Loss: 0.2087, Val Loss: 0.1621\n",
      "Epoch 8/100, Loss: 0.1891, Val Loss: 0.1535\n",
      "Epoch 9/100, Loss: 0.1829, Val Loss: 0.1516\n",
      "Epoch 10/100, Loss: 0.1810, Val Loss: 0.1581\n",
      "Epoch 11/100, Loss: 0.1800, Val Loss: 0.1659\n",
      "Epoch 12/100, Loss: 0.1804, Val Loss: 0.1766\n",
      "Epoch 13/100, Loss: 0.1849, Val Loss: 0.1874\n",
      "Epoch 14/100, Loss: 0.1905, Val Loss: 0.1940\n",
      "Epoch 15/100, Loss: 0.1940, Val Loss: 0.1967\n",
      "Epoch 16/100, Loss: 0.1956, Val Loss: 0.1950\n",
      "Epoch 17/100, Loss: 0.1945, Val Loss: 0.1902\n",
      "Epoch 18/100, Loss: 0.1920, Val Loss: 0.1829\n",
      "Epoch 19/100, Loss: 0.1880, Val Loss: 0.1731\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=64, units_per_layer=64, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2700, Val Loss: 27.7654\n",
      "Epoch 2/100, Loss: 28.0259, Val Loss: 2.2666\n",
      "Epoch 3/100, Loss: 1.9593, Val Loss: 0.2376\n",
      "Epoch 4/100, Loss: 0.2655, Val Loss: 0.2477\n",
      "Epoch 5/100, Loss: 0.2732, Val Loss: 0.2319\n",
      "Epoch 6/100, Loss: 0.2588, Val Loss: 0.2055\n",
      "Epoch 7/100, Loss: 0.2345, Val Loss: 0.1787\n",
      "Epoch 8/100, Loss: 0.2064, Val Loss: 0.1604\n",
      "Epoch 9/100, Loss: 0.1879, Val Loss: 0.1523\n",
      "Epoch 10/100, Loss: 0.1823, Val Loss: 0.1530\n",
      "Epoch 11/100, Loss: 0.1807, Val Loss: 0.1596\n",
      "Epoch 12/100, Loss: 0.1798, Val Loss: 0.1684\n",
      "Epoch 13/100, Loss: 0.1815, Val Loss: 0.1811\n",
      "Epoch 14/100, Loss: 0.1871, Val Loss: 0.1914\n",
      "Epoch 15/100, Loss: 0.1926, Val Loss: 0.1980\n",
      "Epoch 16/100, Loss: 0.1966, Val Loss: 0.1997\n",
      "Epoch 17/100, Loss: 0.1980, Val Loss: 0.1972\n",
      "Epoch 18/100, Loss: 0.1959, Val Loss: 0.1912\n",
      "Epoch 19/100, Loss: 0.1925, Val Loss: 0.1832\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=64, units_per_layer=64, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.4561, Val Loss: 19.3848\n",
      "Epoch 2/100, Loss: 20.7657, Val Loss: 1.0637\n",
      "Epoch 3/100, Loss: 0.9058, Val Loss: 1.5257\n",
      "Epoch 4/100, Loss: 1.5749, Val Loss: 1.4376\n",
      "Epoch 5/100, Loss: 1.5727, Val Loss: 0.8138\n",
      "Epoch 6/100, Loss: 0.9134, Val Loss: 0.2853\n",
      "Epoch 7/100, Loss: 0.3198, Val Loss: 0.1997\n",
      "Epoch 8/100, Loss: 0.2293, Val Loss: 0.1682\n",
      "Epoch 9/100, Loss: 0.1949, Val Loss: 0.1550\n",
      "Epoch 10/100, Loss: 0.1838, Val Loss: 0.1520\n",
      "Epoch 11/100, Loss: 0.1809, Val Loss: 0.1601\n",
      "Epoch 12/100, Loss: 0.1797, Val Loss: 0.1723\n",
      "Epoch 13/100, Loss: 0.1834, Val Loss: 0.1905\n",
      "Epoch 14/100, Loss: 0.1921, Val Loss: 0.2046\n",
      "Epoch 15/100, Loss: 0.2021, Val Loss: 0.2135\n",
      "Epoch 16/100, Loss: 0.2100, Val Loss: 0.2175\n",
      "Epoch 17/100, Loss: 0.2132, Val Loss: 0.2159\n",
      "Epoch 18/100, Loss: 0.2119, Val Loss: 0.2098\n",
      "Epoch 19/100, Loss: 0.2070, Val Loss: 0.2010\n",
      "Epoch 20/100, Loss: 0.1991, Val Loss: 0.1896\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=64, units_per_layer=128, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2813, Val Loss: 86.3893\n",
      "Epoch 2/100, Loss: 91.2753, Val Loss: 3.5952\n",
      "Epoch 3/100, Loss: 3.0559, Val Loss: 3.7957\n",
      "Epoch 4/100, Loss: 3.9375, Val Loss: 4.6520\n",
      "Epoch 5/100, Loss: 5.1192, Val Loss: 3.3554\n",
      "Epoch 6/100, Loss: 3.7702, Val Loss: 1.3471\n",
      "Epoch 7/100, Loss: 1.5242, Val Loss: 0.1941\n",
      "Epoch 8/100, Loss: 0.2286, Val Loss: 0.1606\n",
      "Epoch 9/100, Loss: 0.1880, Val Loss: 0.1514\n",
      "Epoch 10/100, Loss: 0.1820, Val Loss: 0.1547\n",
      "Epoch 11/100, Loss: 0.1805, Val Loss: 0.1631\n",
      "Epoch 12/100, Loss: 0.1797, Val Loss: 0.1755\n",
      "Epoch 13/100, Loss: 0.1846, Val Loss: 0.1916\n",
      "Epoch 14/100, Loss: 0.1927, Val Loss: 0.2035\n",
      "Epoch 15/100, Loss: 0.2012, Val Loss: 0.2100\n",
      "Epoch 16/100, Loss: 0.2072, Val Loss: 0.2119\n",
      "Epoch 17/100, Loss: 0.2087, Val Loss: 0.2090\n",
      "Epoch 18/100, Loss: 0.2063, Val Loss: 0.2026\n",
      "Epoch 19/100, Loss: 0.2004, Val Loss: 0.1931\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=64, units_per_layer=128, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.3814, Val Loss: 71.9193\n",
      "Epoch 2/100, Loss: 78.5000, Val Loss: 4.0940\n",
      "Epoch 3/100, Loss: 3.8570, Val Loss: 0.2811\n",
      "Epoch 4/100, Loss: 0.3059, Val Loss: 0.2816\n",
      "Epoch 5/100, Loss: 0.3063, Val Loss: 0.2573\n",
      "Epoch 6/100, Loss: 0.2827, Val Loss: 0.2212\n",
      "Epoch 7/100, Loss: 0.2491, Val Loss: 0.1876\n",
      "Epoch 8/100, Loss: 0.2160, Val Loss: 0.1631\n",
      "Epoch 9/100, Loss: 0.1900, Val Loss: 0.1533\n",
      "Epoch 10/100, Loss: 0.1828, Val Loss: 0.1527\n",
      "Epoch 11/100, Loss: 0.1808, Val Loss: 0.1599\n",
      "Epoch 12/100, Loss: 0.1797, Val Loss: 0.1699\n",
      "Epoch 13/100, Loss: 0.1823, Val Loss: 0.1849\n",
      "Epoch 14/100, Loss: 0.1891, Val Loss: 0.1969\n",
      "Epoch 15/100, Loss: 0.1957, Val Loss: 0.2042\n",
      "Epoch 16/100, Loss: 0.2018, Val Loss: 0.2065\n",
      "Epoch 17/100, Loss: 0.2040, Val Loss: 0.2044\n",
      "Epoch 18/100, Loss: 0.2020, Val Loss: 0.1984\n",
      "Epoch 19/100, Loss: 0.1969, Val Loss: 0.1893\n",
      "Epoch 20/100, Loss: 0.1915, Val Loss: 0.1786\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=128, units_per_layer=32, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.4319, Val Loss: 7.3292\n",
      "Epoch 2/100, Loss: 7.5279, Val Loss: 0.8665\n",
      "Epoch 3/100, Loss: 0.7687, Val Loss: 0.1927\n",
      "Epoch 4/100, Loss: 0.2293, Val Loss: 0.2226\n",
      "Epoch 5/100, Loss: 0.2503, Val Loss: 0.2127\n",
      "Epoch 6/100, Loss: 0.2413, Val Loss: 0.1932\n",
      "Epoch 7/100, Loss: 0.2224, Val Loss: 0.1718\n",
      "Epoch 8/100, Loss: 0.1988, Val Loss: 0.1588\n",
      "Epoch 9/100, Loss: 0.1865, Val Loss: 0.1519\n",
      "Epoch 10/100, Loss: 0.1822, Val Loss: 0.1527\n",
      "Epoch 11/100, Loss: 0.1808, Val Loss: 0.1585\n",
      "Epoch 12/100, Loss: 0.1799, Val Loss: 0.1657\n",
      "Epoch 13/100, Loss: 0.1803, Val Loss: 0.1751\n",
      "Epoch 14/100, Loss: 0.1844, Val Loss: 0.1856\n",
      "Epoch 15/100, Loss: 0.1895, Val Loss: 0.1922\n",
      "Epoch 16/100, Loss: 0.1930, Val Loss: 0.1953\n",
      "Epoch 17/100, Loss: 0.1947, Val Loss: 0.1953\n",
      "Epoch 18/100, Loss: 0.1947, Val Loss: 0.1924\n",
      "Epoch 19/100, Loss: 0.1932, Val Loss: 0.1870\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=128, units_per_layer=32, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.4321, Val Loss: 4.8546\n",
      "Epoch 2/100, Loss: 5.1371, Val Loss: 0.4637\n",
      "Epoch 3/100, Loss: 0.4077, Val Loss: 0.2372\n",
      "Epoch 4/100, Loss: 0.2616, Val Loss: 0.2657\n",
      "Epoch 5/100, Loss: 0.2882, Val Loss: 0.2456\n",
      "Epoch 6/100, Loss: 0.2712, Val Loss: 0.2129\n",
      "Epoch 7/100, Loss: 0.2415, Val Loss: 0.1822\n",
      "Epoch 8/100, Loss: 0.2101, Val Loss: 0.1612\n",
      "Epoch 9/100, Loss: 0.1884, Val Loss: 0.1524\n",
      "Epoch 10/100, Loss: 0.1824, Val Loss: 0.1532\n",
      "Epoch 11/100, Loss: 0.1807, Val Loss: 0.1601\n",
      "Epoch 12/100, Loss: 0.1797, Val Loss: 0.1699\n",
      "Epoch 13/100, Loss: 0.1823, Val Loss: 0.1842\n",
      "Epoch 14/100, Loss: 0.1888, Val Loss: 0.1954\n",
      "Epoch 15/100, Loss: 0.1947, Val Loss: 0.2028\n",
      "Epoch 16/100, Loss: 0.2005, Val Loss: 0.2055\n",
      "Epoch 17/100, Loss: 0.2030, Val Loss: 0.2034\n",
      "Epoch 18/100, Loss: 0.2011, Val Loss: 0.1973\n",
      "Epoch 19/100, Loss: 0.1960, Val Loss: 0.1883\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=128, units_per_layer=64, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2610, Val Loss: 21.6287\n",
      "Epoch 2/100, Loss: 25.4471, Val Loss: 0.3358\n",
      "Epoch 3/100, Loss: 0.4703, Val Loss: 2.7347\n",
      "Epoch 4/100, Loss: 3.1412, Val Loss: 2.4962\n",
      "Epoch 5/100, Loss: 2.9386, Val Loss: 1.4266\n",
      "Epoch 6/100, Loss: 1.6734, Val Loss: 0.4243\n",
      "Epoch 7/100, Loss: 0.4773, Val Loss: 0.1649\n",
      "Epoch 8/100, Loss: 0.1910, Val Loss: 0.1521\n",
      "Epoch 9/100, Loss: 0.1823, Val Loss: 0.1550\n",
      "Epoch 10/100, Loss: 0.1805, Val Loss: 0.1648\n",
      "Epoch 11/100, Loss: 0.1800, Val Loss: 0.1808\n",
      "Epoch 12/100, Loss: 0.1869, Val Loss: 0.1971\n",
      "Epoch 13/100, Loss: 0.1958, Val Loss: 0.2079\n",
      "Epoch 14/100, Loss: 0.2052, Val Loss: 0.2134\n",
      "Epoch 15/100, Loss: 0.2099, Val Loss: 0.2133\n",
      "Epoch 16/100, Loss: 0.2098, Val Loss: 0.2082\n",
      "Epoch 17/100, Loss: 0.2055, Val Loss: 0.1997\n",
      "Epoch 18/100, Loss: 0.1980, Val Loss: 0.1881\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=128, units_per_layer=64, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.2526, Val Loss: 11.9667\n",
      "Epoch 2/100, Loss: 12.9170, Val Loss: 0.2452\n",
      "Epoch 3/100, Loss: 0.2886, Val Loss: 0.4810\n",
      "Epoch 4/100, Loss: 0.5166, Val Loss: 0.3338\n",
      "Epoch 5/100, Loss: 0.2101, Val Loss: 0.1559\n",
      "Epoch 6/100, Loss: 0.2090, Val Loss: 0.1456\n",
      "Epoch 7/100, Loss: 0.1726, Val Loss: 0.1540\n",
      "Epoch 8/100, Loss: 0.1965, Val Loss: 0.1488\n",
      "Epoch 9/100, Loss: 0.1744, Val Loss: 0.1500\n",
      "Epoch 10/100, Loss: 0.1863, Val Loss: 0.1500\n",
      "Epoch 11/100, Loss: 0.1866, Val Loss: 0.1502\n",
      "Epoch 12/100, Loss: 0.1827, Val Loss: 0.1505\n",
      "Epoch 13/100, Loss: 0.1820, Val Loss: 0.1511\n",
      "Epoch 14/100, Loss: 0.1812, Val Loss: 0.1518\n",
      "Epoch 15/100, Loss: 0.1809, Val Loss: 0.1526\n",
      "Epoch 16/100, Loss: 0.1808, Val Loss: 0.1534\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=128, units_per_layer=128, dropout_rate=0.3\n",
      "Epoch 1/100, Loss: 0.2180, Val Loss: 78.6409\n",
      "Epoch 2/100, Loss: 82.5424, Val Loss: 2.0830\n",
      "Epoch 3/100, Loss: 1.6447, Val Loss: 8.6314\n",
      "Epoch 4/100, Loss: 9.0139, Val Loss: 7.3488\n",
      "Epoch 5/100, Loss: 8.2350, Val Loss: 3.3219\n",
      "Epoch 6/100, Loss: 3.8280, Val Loss: 0.7559\n",
      "Epoch 7/100, Loss: 0.9349, Val Loss: 0.1711\n",
      "Epoch 8/100, Loss: 0.1980, Val Loss: 0.1575\n",
      "Epoch 9/100, Loss: 0.1856, Val Loss: 0.1500\n",
      "Epoch 10/100, Loss: 0.1814, Val Loss: 0.1561\n",
      "Epoch 11/100, Loss: 0.1803, Val Loss: 0.1642\n",
      "Epoch 12/100, Loss: 0.1799, Val Loss: 0.1757\n",
      "Epoch 13/100, Loss: 0.1846, Val Loss: 0.1897\n",
      "Epoch 14/100, Loss: 0.1917, Val Loss: 0.1997\n",
      "Epoch 15/100, Loss: 0.1980, Val Loss: 0.2049\n",
      "Epoch 16/100, Loss: 0.2024, Val Loss: 0.2052\n",
      "Epoch 17/100, Loss: 0.2027, Val Loss: 0.2012\n",
      "Epoch 18/100, Loss: 0.1992, Val Loss: 0.1939\n",
      "Epoch 19/100, Loss: 0.1939, Val Loss: 0.1848\n",
      "Early stopping!\n",
      "Training with lr=0.1, batch_size=128, units_per_layer=128, dropout_rate=0.5\n",
      "Epoch 1/100, Loss: 0.2177, Val Loss: 3.1954\n",
      "Epoch 2/100, Loss: 3.7425, Val Loss: 20.8171\n",
      "Epoch 3/100, Loss: 25.1670, Val Loss: 69.5853\n",
      "Epoch 4/100, Loss: 78.7375, Val Loss: 23.2192\n",
      "Epoch 5/100, Loss: 25.6741, Val Loss: 1.9371\n",
      "Epoch 6/100, Loss: 1.5525, Val Loss: 0.9381\n",
      "Epoch 7/100, Loss: 1.1536, Val Loss: 0.5500\n",
      "Epoch 8/100, Loss: 0.8063, Val Loss: 0.4835\n",
      "Epoch 9/100, Loss: 0.3700, Val Loss: 0.3653\n",
      "Epoch 10/100, Loss: 0.3345, Val Loss: 0.1386\n",
      "Epoch 11/100, Loss: 0.1943, Val Loss: 0.1702\n",
      "Epoch 12/100, Loss: 0.1986, Val Loss: 0.1812\n",
      "Epoch 13/100, Loss: 0.1899, Val Loss: 0.1688\n",
      "Epoch 14/100, Loss: 0.1813, Val Loss: 0.1688\n",
      "Epoch 15/100, Loss: 0.1817, Val Loss: 0.1683\n",
      "Epoch 16/100, Loss: 0.1814, Val Loss: 0.1669\n",
      "Epoch 17/100, Loss: 0.1807, Val Loss: 0.1648\n",
      "Epoch 18/100, Loss: 0.1801, Val Loss: 0.1626\n",
      "Epoch 19/100, Loss: 0.1797, Val Loss: 0.1603\n",
      "Epoch 20/100, Loss: 0.1797, Val Loss: 0.1590\n",
      "Early stopping!\n",
      "Best hyperparameters: lr=0.01, batch_size=128, units_per_layer=64, dropout_rate=0.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 하이퍼파라미터 튜닝을 위한 반복 구조\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for units_per_layer in units_per_layers:\n",
    "            for dropout_rate in dropout_rates:\n",
    "                print(f'Training with lr={lr}, batch_size={batch_size}, units_per_layer={units_per_layer}, dropout_rate={dropout_rate}')\n",
    "                \n",
    "                # 데이터 로더 생성 (batch_size 변경)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "                \n",
    "                # 모델 초기화\n",
    "                input_dim = x_train.shape[1]\n",
    "                model = LinearNN(input_dim, units_per_layer, dropout_rate).to(device)\n",
    "                \n",
    "                # 옵티마이저 및 손실 함수 설정\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                criterion = nn.L1Loss() # 회귀 문제에 적합한 손실 함수 사용\n",
    "\n",
    "                # Early Stopping 초기화\n",
    "                patience = 10\n",
    "                early_stopping_counter = 0\n",
    "                current_best_val_loss = float('inf')\n",
    "                \n",
    "                # 학습 함수 정의\n",
    "                def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, patience, device):\n",
    "                    model.train()  # 학습 모드로 설정\n",
    "                    for epoch in range(epochs):\n",
    "                        epoch_loss = 0.0\n",
    "                        for inputs, labels in train_loader:\n",
    "                            inputs, labels = inputs.to(device), labels.to(device)\n",
    "                            optimizer.zero_grad()  # 그래디언트 초기화\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            loss.backward()  # 역전파\n",
    "                            optimizer.step()  # 파라미터 업데이트\n",
    "                            epoch_loss += loss.item()\n",
    "                        \n",
    "                        # 검증 손실 계산\n",
    "                        val_loss = validate_model(model, val_loader, criterion, device)\n",
    "                        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')\n",
    "                        \n",
    "                        # Early Stopping 확인\n",
    "                        global early_stopping_counter, current_best_val_loss\n",
    "                        if val_loss < current_best_val_loss:\n",
    "                            current_best_val_loss = val_loss\n",
    "                            early_stopping_counter = 0  # 카운터 초기화\n",
    "                        else:\n",
    "                            early_stopping_counter += 1\n",
    "                        \n",
    "                        if early_stopping_counter >= patience:\n",
    "                            print(\"Early stopping!\")\n",
    "                            break\n",
    "\n",
    "                # 검증 함수 정의\n",
    "                def validate_model(model, val_loader, criterion, device):\n",
    "                    model.eval()  # 평가 모드로 설정\n",
    "                    val_loss = 0.0\n",
    "                    with torch.no_grad():\n",
    "                        for inputs, labels in val_loader:\n",
    "                            inputs, labels = inputs.to(device), labels.to(device)\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            val_loss += loss.item()\n",
    "                    return val_loss / len(val_loader)\n",
    "\n",
    "                # 모델 학습\n",
    "                train_model(model, train_loader, val_loader, criterion, optimizer, epochs=100, patience=patience, device=device)\n",
    "\n",
    "                # 최적의 하이퍼파라미터 업데이트\n",
    "                if current_best_val_loss < best_val_loss:\n",
    "                    best_val_loss = current_best_val_loss\n",
    "                    best_hyperparams = (lr, batch_size, units_per_layer, dropout_rate)\n",
    "                    print(f'New best validation loss: {best_val_loss:.4f}')\n",
    "\n",
    "\n",
    "print(f'Best hyperparameters: lr={best_hyperparams[0]}, batch_size={best_hyperparams[1]}, units_per_layer={best_hyperparams[2]}, dropout_rate={best_hyperparams[3]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84ccb776-30b6-4ca4-b21c-faa540cb4e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.2476, Val Loss: 0.1989\n",
      "Early stopping!\n",
      "MSE: 0.0824\n",
      "MAE: 0.2236\n",
      "RMSE: 0.2870\n",
      "MAPE: 2.1363\n",
      "R-squared (R²): 0.2044\n",
      "Predictions:  [0.42258024 0.40179813 0.22891968 0.50939775 0.5632059  0.47730932\n",
      " 0.44034815 0.610952   0.56968766 0.26392597]\n",
      "Actuals:  [1.         0.02058912 0.08884688 0.598165   0.5914565  0.72297\n",
      " 0.23890625 0.5970628  0.08134608 0.15385374]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 최적의 하이퍼파라미터로 모델 평가\n",
    "input_dim = x_train.shape[1]\n",
    "best_model = LinearNN(input_dim, best_hyperparams[2], best_hyperparams[3]).to(device)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_hyperparams[0])\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_hyperparams[1], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_hyperparams[1], shuffle=False)\n",
    "\n",
    "# 최적의 모델 학습\n",
    "train_model(best_model, train_loader, val_loader, criterion, optimizer, epochs=100, patience=patience, device=device)\n",
    "\n",
    "# 모델 평가\n",
    "\n",
    "# 모델 평가 함수 정의 (회귀 지표 포함)\n",
    "def evaluate_model_and_metrics(model, test_loader, device):\n",
    "    model.eval()  # 평가 모드로 설정\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화 (평가 시에는 필요 없음)\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # 예측값 계산\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 예측값과 실제값을 리스트에 저장\n",
    "            predictions.extend(outputs.cpu().numpy())  # GPU에서 CPU로 이동 후 numpy로 변환\n",
    "            actuals.extend(labels.cpu().numpy())  # GPU에서 CPU로 이동 후 numpy로 변환\n",
    "    \n",
    "    # NumPy 배열로 변환\n",
    "    predictions = np.array(predictions).flatten()  # 예측값을 1차원 배열로 변환\n",
    "    actuals = np.array(actuals).flatten()  # 실제값을 1차원 배열로 변환\n",
    "    \n",
    "    # 회귀 성능 지표 계산\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    \n",
    "    print(f'MSE: {mse:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'MAPE: {mape:.4f}')\n",
    "    print(f'R-squared (R²): {r2:.4f}')\n",
    "    \n",
    "    return predictions, actuals\n",
    "\n",
    "# Test DataLoader 생성\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 모델 평가 및 예측 결과 확인\n",
    "predictions, actuals = evaluate_model_and_metrics(best_model, test_loader, device)\n",
    "\n",
    "# 결과 출력\n",
    "# 예측 결과 일부를 출력\n",
    "print(\"Predictions: \", predictions[:10])\n",
    "print(\"Actuals: \", actuals[:10])\n",
    "\n",
    "# 예측 결과 시각화 (예: 실제값 vs 예측값)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(actuals, predictions, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c34ada-56d3-479a-b1de-d91e986a1b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
